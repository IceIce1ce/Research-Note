<h2>1. Abstract</h2>
This survey provides a comprehensive review of various anomaly detection and recognition methods. Researchers will get a better perspective of anomaly detection task with GAN approach, fine-tuned approach and keyframes extraction plus shallow network approach and also their issues as well.
<h2>2. Introduction</h2>
Three categories: one is an autoencoder in which model tries to learn how to generate a normal frame and not an abnormal frame. The second category uses different pre-trained model which already trained on video data and ready to use as a temporal features extraction. The last category has unique techniques like weakly labels and keyframes extraction.
<h2>3. Literature review</h2>
Two groups: traditional handmade feature-based techniques and deep learning feature-based methods for recognising anomalous events. Anomaly detection used to be very reliant on low-level, manually created feature-based approaches. These approaches are typically based on 3 stages. First, recognising patterns from the training set. Second, differentiating encoding normal occurrences distribution known as feature learning. Last one is separation of clusters and outliers which are recognised as aberrant events. 
<h2>4. EADN: efficient deep learning model for anomaly</h2>
Classification of anomaly and detection task contains majorly 3 parts. First, keyframes extraction. Second, feature extraction. Last one is learning and temporal anomaly classification. In the first part, shot boundary detection algorithm helps to segment key frames. In the second step, these key frames are passed through lightweight CNN to learn spatiotemporal features. After that as the third step to learn temporal features from an ordered key frames LSTM cells are used. $LW_{CNN}$ took frames that are in sequential manner to detect action and movement. At last, trained $LW_{CNN}$ with LSTM cells network is utilized to classify abnormal action in video's segmented shot.
<h3>4.1 Network architecture</h3>
Use two time-distributed MaxPool2D layers following time-distribution 2D Conv2D. Every time-distributed Conv2D layer with kernel size of 3 $\times$ 3 and stride size of 2 $\times$ 2 go along with ReLU. To reduce size of network, it has time distributed MaxPool2D layer which has 2 $\times$ 2 strides after the second and third convolution layer. Same-padding approach used to build time-distributed Conv2D layer to stop leaving knowledge at the edge of input frame. Model architecture contains 64 featured maps in first convolution layer in the starting which followed by second layers which contain same number of feature maps. There are total of 128 feature maps in final convolutional layer. A segmented shot's pre-processed frame is fed into proposed model as input. First, $LW_{CNN}$ system takes out spatial features and then by giving sequence of spatial features in LSTM captures temporal features. Last time step output of LSTM passes into fully connected dense layer and at last for prediction, it passes through softmax. For spatial features capturing, it passed into $LW_{CNN}$ one by one frame wise. Moreover, input frame at time $t$ is $f_t$ similarly $f_{t + 1}$ and $f_{t + n}$ feed into $LW_{CNN}$, by this every frame converts into series of sequential spatial feature: $F_t = LW_{CNN}(f_t, f_{t + 1}, f_{t + n})$, where $F_t$ represents spatial feature series. But, LSTM input is feed with representation series of spatial features to learn temporal features $H_{t + n}$: $H_{t + n} = LSTM(F_t)$. At ongoing time step $t$, hidden state of LSTM represented by $H_t$ and similarly $H_{t - 1}$ represents previous time step $t$ - 1 hidden state. Information of previous time step is passed into current time step as an input. Hidden state $H_{t + n}$ is output of last time step passed as input to the next fully connected dense layer. At last, softmax layer is feeded with fully connected layer's output: $y_j = softmax(H_{t + n})$.
<h2>5. Anomaly recognition from surveillance videos using 3D convolution neural network</h2>
First, all frames were converted into 3D cubes to train model after pre-processing. A concatenation of sequential frames is passed into model as input. To not lose channel information, frames are passed in their original state and not in grayscale. The pre-trained model has convolution, pooling and fully connected layers, researcher also added batch norm. 3D cubes are passed to 3D convNets as an input to take out spatiotemporal features and as input spatiotemporal is passed into a fully connected layer and then fully connected layer output passed to SoftMax. 
<h3>5.1 Data preparation</h3>
In the pre-processing step, first sequential frames are extracted from each video. These sequential frames are changed to 170 $\times$ 170 dimensions. Then, all frames' pixel values are scaled from 0 to 1. Later on, each class's frames are transformed using spatial augmentation. After implementing augmentation method, certain length of 3D cubes prepared from each video to pass temporal and spatial features into fine-tuned deep model.
<h3>5.2 Spatial augmentation</h3>
Augmentation applied to UCF-Crime dataset in such a way that first $n$ number of frames $V = \{f_1,...,f_n\}$ are obtained from each video then augmented using vertical and horizontal flip approach. Apply other augmentation (rotation) on video datasets occasionally results in loss in temporal information and adds noise to the dataset.
<h2>6. Anomaly detection in surveillance video based on bidirectional prediction</h2>
<h3>6.1 Network structure</h3>
Firstly, U-Net opted as backbone network for both forward predictive subnetwork $G_F$ and backward predictive subnetwork $G_B$. The setting of hyperparameters is detailed in P-GAN and both subnetworks $G_F$ and $G_B$ are identical in structure. The current target frames are denoted as $I_t$ while preceding k frames are denoted as $I_{t - k},...,I_{t - 1}$. Then, dimensional spatiotemporal tensors of each video of k frames are prepared as inputs and feed these tensors to the forward prediction subnetwork $G_F$. The produced frame that corresponds to $G_F$ is labeled as $\hat{I}^F_t$. Similarly, next followed k frames $I_{t + 1},...,I_{t + k}$ and their spatiotemporal tensors are prepared as inputs to backward prediction subnetwork $G_B$. The corresponding generated frame is $\hat{I}^B_t$.
<h3>6.2 Loss function</h3>
$MSE_F$ for a single prediction network is defined using target frame $I$ and forward prediction frame $\hat{I}^F$. Similarly, definition of $MSE_B$ has same meaning: $MSE_F = \frac{1}{MN}\sum_{i = 1}^M\sum{j = 1}^N[I^F(i, j) - I(i, j)]^2, MSE_B = \frac{1}{MN}\sum_{i = 1}^M\sum{j = 1}^N[I^B(i, j) - I(i, j)]^2$. Mean square error between backward and forward prediction frame $MSE_M$ used as loss function of bidirectional prediction model: $MSE_M = \frac{1}{MN}\sum_{i = 1}^M\sum_{j = 1}^N[I^F(i, j) - I^B(i, j)]^2$. Finally, $L_F$ is the joint loss of forward prediction subnetwork $G_F: L_F = MSE_F + \lambda_FMSE_M$ and $L_B$ is the joint loss of backward prediction subnetwork $G_B: L_B = MSE_B + \lambda_BMSE_M$.
<h3>6.3 Abnormality estimation</h3>
PSNR is used to calculate anomaly score of current frame $I_t$ between $\hat{I}^F_t$ and backward prediction $\hat{I}^B_t$ in the testing process: $PSNR = 10\log_{10}(\frac{I^2_{max}}{MSE_M})$, where $I_{max}$ is the highest pixel value possible for current frame which is typically 255. Higher PSNR --> lower anomaly score. Another approach for anomaly score estimation is blocking strategy.
<h2>7. Future research directions</h2>
- A better temporal features extraction method is required. 
- Model takes frames as input and do needed maths to predict whether given frames have anomaly or not. But to identify which frame is dependent on another frame to classify video and which frame is more contributing in classification for that a self-attention mechanism and vision transformer could help.
- Take anomaly recognition problem as a video caption problem and then based on text prediction a text classification model can be used to classify anomaly category.