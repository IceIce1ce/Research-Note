<h2>1. Abstract</h2>
Address road anomaly detection by formulating it as a classification problem and applying deep learning approaches to solve it. The paper pays a close attention to pattern representation, and process three sets of numeric features for representing road conditions. Also, three deep learning approaches: DFN, CNN and RNN are considered to tackle the classification problem.
<h2>2. Introduction</h2>
Main contributions:
- The eight types of road (pavement) anomalies are specified from the perspective of a vehicle. 
- The three sets of numerical features for representing road conditions are proposed.
- Road anomaly detection is formulated as a classification problem and DFN, CNN, RNN are used to solve it.
<h2>3. Related work</h2>
Road anomaly detectors based on support vector machine. Robust features of road anomaly are extracted by applying signal processing techniques, such as wavelet decomposition, resampling and thresholding to acceleration signal or other types of inertial signals of vehicle response. Instead of using vehicle response signals, researchers also take advantage of image data to perform road anomaly detection directly. In addition, machine learning approaches such as SVM, special designed CNN and conventional CNN are also investigated and integrated with traditional image process methods. However, these approaches may fail due to potential issues with lighting and weather conditions.
<h2>4. problem formulation</h2>
The anomaly detection is formulated as a binary classification problem, which differentiates the normal road condition from an anomalous road condition of any of the eight types in set $\mathbb{RA}$ = {pothole, bump, gravel, cobblestone, broken concrete, curb impact, the road condition that causes high wheel impact, the road condition that cause severe vehicle body twist}. Let $X_R = {x_R^{k} \in \mathbb{R}^m: k \in \{1, 2,...,N\}}$ be a multivariate time series which is a sequence with length $N$. It includes the collection of the raw samples corresponding to the natural samples obtained through real-time sampling of the selected $m$ vehicle response signals which are assumed to be sampled at the same frequency. Raw sample $x_R^{(k)}$ is a $m$-tuple and sampled at time instant $k$, which is also regarded as a column vector: $x_R^{(k)} = [x_{1R}^{(k)}, x_{2R}^{(k)},...,x_{mR}^{(k)}]^T, k \in \{1,...N\}$. The ground truth label of $x_R^{(k)}$ is defined: $y_R^{(k)} = L_R[x_R^{(k)}]$, where $L_R$ is labeling function. Hence, the raw label set corresponds to $X_R$ follows: $Y_R = {y_R^{(k)}: k \in \{1, 2,...N\}}$. Sliding window techniques are usually used to divide time-series into discrete segments in order to reveal underlying properties of its source. Hence, a sliding window with fixed length $TR$ is applied to $X_R$. Let $X_T = \{x_T^{(i)} \in \mathbb{R}^{m \times TR}: i \in \{0,1,...(N_T - 1)\}\}$ be the set resulting from applying the sliding window, whose elements are segments of $X_R$. Denote segment $x_T^{(i)}$ as raw training sample. Refer $TR$ as training sample to raw sample ratio since one (raw) training sample for learning models originates from $TR$ consecutive raw samples: $x_T^{(i)} = [x_R^{(i * (TR - N_{ors}) + 1)}, x_R^{(i * (TR - N_{ors}) + 2)},...,x_R^{(i * (TR - N_{ors}) + TR)}]$, where $TR \leq N$ and $N_{ors}$ is the number of overlapped raw samples between $x_T^{(i)}$ and $x_T^{(i + 1)}$. Therefore, the total number of raw training samples $N_T$ can be computed from the total number of raw samples $N$: $N_T = \lfloor\frac{N - TR}{TR - N_{ors}}\rfloor + 1$. To generate the final training sample set $X_T^f$ for the learning algorithms, a function $modelDependConversion()$ is used such that $x_T^{(i)^f} = modelDependConversion(x_T^{(i)}), X_T^f = \{x_T^{(i)^f}: i \in \{0,1,...,(N_T - 1)\}\}$, where $x_T^{(i)^f} \in X_T^f$ is interpreted as the pattern representation of road conditions. Define the corresponding ground truth label of $x_T^{(i)^f}$ as: $y_T^{(i)} = L_T[y_R^{(i * (TR - N_{ors}) + 1)}, y_R^{(i * (TR - N_{ors}) + 2)},...,y_R^{(i * (TR - N_{ors}) + TR)}]$, where $L_T$ is a projection function. The labeled set associated with the final training sample set $X_T^f$ is defined as $Y_T$ called training label set: $Y_T = \{y_T^{(i)}: i \in \{0,1,...,(N_T - 1)\}\}$. The resulting dataset $\mathbb{SL}$ is then a set of pairs as follows: $\mathbb{SL} = \{<x_T^{(i)^f}, y_T^{(i)}>: i \in \{0,1,...,(N_T - 1)\}\}$. For the binary classification of road anomaly detection: $Y_T = Y_R = \{0, 1\}$, where label 0 means normal road condition and 1 otherwise. The problem to be solve here is to find a parameterized mapping with parameters $\theta$ taking values from a parameter set $\clubsuit$: $h_\theta: X_T^f \rightarrow Y_T, \theta \in \clubsuit$, where $h_\theta$ has a given structure such that the cost function: $J(\theta) = \sum_{i = 0}^{N_T - 1}H[h_\theta(x_T^{(i)^f}, y_T^{(i)}]$, where $H$ is a function to measure the dissimilarity between the estimated label $h_\theta(x_T^{(i)^f})$ and ground truth $y_T^{(i)}$ is minimized. The estimated label $\hat{y}$ is computed: $\hat{y}$ = 1 if $h_{\theta^*} \geq 0.5$ and 0 otherwise.
<h2>5. Dataset preparation</h2>
<h3>5.1 Pattern representation</h3>
Three sets of signals (numerical features) representing road conditions:
- Standard set: $\mathbb{S}$ = {Vehicle Longitudinal Acceleration, Vehicle Vertical Acceleration, Vehicle Lateral Acceleration, Vehicle Roll Rate, Vehicle Yaw Rate, Vehicle Speed}.
- Performance set: $\mathbb{P}$ = {Standard set, Rotation speeds of the 4 wheels}.
- All-signal set: $\mathbb{A}$ = {Performance set, Spindle responses of the 4 spindles, Shock absorber responses of the 4 absorbers}.
<h3>5.2 Dataset construction</h3>
- Raw sample labeling $L_R$: $L_r[x_R^{(k)}] = 1$ if $\exists j, s.t. x_{jR}^{(k)}$ is different from norm and 0 otherwise, where $j$ indexes the $j$-th element of tuple.
- Training sample labeling $L_T$: $y_T^{(i)} = L_T[y_R^{(i * (n - N_{ors})} + 1),...,y_R^{(i * (n - N_{ors}) + n)}] = 1$ if majority of the raw labels $y_R$ is 1 and 0 otherwise. Set $N_{ors} = TR - 1$ and treat as a hyper-parameter owing to their coupled and cooperative behavior. $\mathbb{TR} = \{1, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56\}$. By such a setting, expect to see how the length of a sliding window, $TR$, effects the performance of a model.
<h2>6. Modeling road anomaly</h2>
<h3>6.1 Deep feedforward network</h3>
The output of $modelDependConversion()$ is governed by: $x_T^{(i)^f} = \frac{\sum_{i = 0}^{TR - 1}x_T^{(i)}[:, k]}{TR}$, where $x_T^{(i)}[:, k]$ is the $k$-th column vector of $x_T^{(i)}$. There are 75 nodes in the first hidden layer, 50 nodes in the second hidden layer and 15 nodes in the last hidden layer. The dimension of the input layer is governed by the number of signals (features) used. The computation of any hidden node is given: $node_{out} = max(0, W^Tnode_{inp} + b)$, where dedicated parameters $W$ and $b$ are associated with the node, $node_{inp}$ is the input of the node and $node_{out}$ is the output of the node. For the output node, its computation is given by: $\hat{y} = node_{out} = 1$ if $sigmoid(W_0^Tnode_{inp} + b_0 \geq 0.5)$ and 0 otherwise, where $W_0$ and $b_0$ are parameters associated with the output node. 
<h3>6.2 Convolutional neural network</h3>
The output of $modelDependConversion()$ is given: $x_T^{(i)^f} = reshape(x_T^{(i)})$.
<h3>6.3 Recurrent neural network</h3>
The output of $modelDependConversion()$ is given: $x_T^{(i)^f} = x_T^{(i)} = modelDependConversion(x_T^{(i)})$.
<h2>7. Metrics</h2>
Accuracy.