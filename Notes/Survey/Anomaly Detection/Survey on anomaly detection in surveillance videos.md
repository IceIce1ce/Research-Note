<h2>1. Abstract</h2>
The major challenges in Anomaly detection are extraction of appropriate features, addressing normal behaviour, environment divergence and occurrence of abnormal events in sparse manner. In this paper, a comparative study of different anomaly detection methods in deep learning and representation learning is performed and the limitations of each method are listed.
<h2>2. Introduction</h2>
In this paper, discuss about eight different methods, of which four methods falls under deep learning and others in representation learning. Deep learning models: Incremental spatio-temporal learner approach, unsupervised spectral mapping, multilayer perceptron RNN and GE based promotion method. Representation learning models: optical flow based convolutional autoencoder, detection using low dimension descriptor, local motion based joint video representation and low rank dictionary learning.
<h2>3. Video anomaly types and approaches</h2>
Anomalies are of four types: local anomaly, global anomaly, interaction anomaly and point anomaly. Local anomalies mean behaviour of an object which differ from neighbourhood objects. Global anomalies are group of abnormal behaviour also termed as crowd anomalies. A difference in single data point from other dataset is a point anomaly and the unusual interaction between human individuals is interaction anomalies.
<h3>3.1 Incremental spatio-temporal learner approach</h3>
ISTL is an unsupervised deep learning method using active learning with fuzzy aggregation. The three phases of ISTL includes: spatio temporal learning, anomaly detection and localization, active learning with fuzzy aggregation. Spatial temporal learning refers to continuous learning process with the help of both locations based and time interval information. Spatio temporal autoencoder is developed for learning both motion and appearance from the input image. The pre-processing step is performed in input layer to increase the learning of autoencoder, optical low based convoltional autoencoder. dimension of each frame is reduced by converting frames in to grayscale. Convolutional layers are responsible for preserving spatial relationship between frames. The ISTL layers model contain two convolutional layer and two deconvolutional layers and each layer have set of filters for feature extraction. Convolutional LSTM layers are used for capturing temporal behaviour of input data. The trained autoencoder does not have the ability to reconstruct the abnormal scenes from the input video. Therefore, reconstruction error represents score value of abnormal temporal cuboid. The reconstruction error is computed: E = $\sum_{i = 0}^T\sum_{j = 0}^W\sum_{k = 0}^h\sqrt[2]{\varphi(i, j, k)}, \varphi(i, j, k) = |\mathrm{X}_{i, j, k} - \overline{\mathrm{X}}_{i, j, k}|^2$, where X and $\overline{\mathrm{X}}$ are temporal cuboid of input and reconstructed image.
<h3>3.2 Detection using optical low and convolutional autoencoder</h3>
Optical flow is applied along with convolutional autoencoder for improving accuracy and reducing computational complexity. The entire framework consists of three different stages. The first stage is pre-processing which helps to extract dense optical flow of frames. The second stage is called Convolutional autoencoder helps to obtain spatial structure of a frames and third stage is convolutional LSTM to learn temporal patterns of frames. The steps included in pre-processing are rescaling and finding optical flow of video frames. After the pre-processing video frames are given to deep learning model f to maintain spatial features of video frames using back propagation algorithm. The model contains two parts spatial encoder and spatial decoder having LSTM layers. The network is trained by normal behaviour and square filters are used to maintain spatial relationship between video frames. Finally, regularity score is computed using multi scale structural similarity method: $r(x) = 1 - \frac{a(x) - min_ta(t)}{max_ta(t) - min_ta(t)}, a(x) = 1 - (MS\_SSIM(x, f_w(x))$, where $a(x)$ is the pixel wise error and $MS\_SSMI$ is the multi-scale structural similarity between frames. To avoid local minima problem, persistence ID algorithm is used.
<h3>3.3 Crowd anomaly detection using low dimensional descriptor</h3>
The three features extracted from the optical flow vector of video frames is used to develop a descriptor. Feature 1: it is used to eliminate perturbations of background area in video frames: $\overline{OF}(i, j, k) = OF(i, j, k)$ if $OF(i, j, k) > T$ and 0 otherwise. Feature 2: the joint entropy method is applied to find the dissimilarities between two consecutive video frames. Feature 3: this feature is used for computing variance in space and time. If a video frame has high variance the degree of crowd desperation is very high. Third feature is computed: $F_3 = \frac{1}{N}\sum_{i = 1}^N(x_i - \mu)^2$. After analysing three feature a post processing is also needed to obtain smooth profile, reducing effect of noise and outlier. SVM is trained with only normal behaviour of crowd and it maps the normal behaviour into hypersphere.
<h3>3.4 Anomaly detection and localization by local motion based joint video representation</h3>
Features are extracted by considering two motion based video descriptor. Spatially localized histogram of optical flow (SL-HOF) and uniform local gradient pattern based optical flow (ULGP-OF) methods are applied to capture spatial and optical flow information. PCA is applied for localization of foreground region. First the training videos are used for extracting both spatial region and local texture with the help of SL-HOF and ULGP-OF descriptors. Both SL-HOF and ULGP-OF algorithms are modelled by one class extreme learning machine (OCELM). To obtain SL-HOF based video representation steps are:
- Videos frames split in to non-overlapping patches.
- Patches having similar spatial location are combined into spatio-temporal cuboid.
- The spatio-temporal cuboid is divided in to different non overlapping $m \times n$ 3D local regions.
- Sub histogram is extracted from $m \times n$ 3D local region.
- All the histogram is combined to obtain SL-HOF feature.
<h4>3.4.1 Motion of local texture</h4>
With PCA and sigmoid transformation, foreground extractor and mapping operation are performed. Foreground objects localization are carried out with binarization method and enhancement of bounding boxes appearance through ULGP-OF descriptor. A low level descriptor code is used for image texture representation. All said features are modelled by OCELM by applying sparse density learning. Furthermore, gaussian kernel based OCELM is implemented for increasing both accuracy and learning speed.
<h3>3.5 Anomaly detection based on low rank dictionary learning</h3>
Features are extracted by using background subtraction and binarization methods. During training stage, a low rank dictionary based similarity matrix is obtained and develop a new optimization method. Low-rank and compact coefficient dictionary learning (LRCCDL) is an iterative process to obtain low rank dictionary and mean vector. Motion information of two frames are obtained by optical flow field by considering both amplitude and directions. In order to detect the anomalous frames, the reconstruction error is computed and compared with the threshold value.
<h3>3.6 Unsupervised spectral mapping and hyperspectral anomaly detection</h3>
During spectral mapping the spectral consistency and gaussianity are added to find out the nonlinear mapping relation between high-dimensional and low-dimensional feature space.
<h4>3.6.1 Spectral mapping</h4>
Let X represent the input HSI with $N \times N$ spectral vectors and contain C dimensions which represents spectral maps in the deep latent space. Spectral mapping encoding (he) and decoding (hd) is defined: he = $f(w_ix_i + b_1)$, hd = $f(w_im_i + b_1)$. SGD is used for optimizing params of spectral mapping autoencoder. 
<h4>3.6.2 Feature selection</h4>
To minimize loss of information, similar points are considered. Selected maps are calculated using: $B = argmax(\epsilon_i \times S_i)$, where $\epsilon_i$ is the density and $S_i$ is the distance between the maps.
<h4>3.6.3 Anomaly detection</h4>
$Q = \alpha_1P_1 + \alpha_2P_2 + \alpha_3P_3$, where $P_1, P_2, P_3$ are called attributes maps of anomalies and $\alpha_1, \alpha_2, \alpha_3$ are params of selected features. Finally, guided filter helps to eliminate cluster anomalies and also reduces false alarm rate.
<h3>3.7 Anomaly detection based on multilayer perceptron RNN</h3>
The input video is given to frame extraction stage for extracting frames contain anomalous information. Then the gaussian mixture model is used for background estimation. After object detection, ROI method is implemented for extracting foreground information from back ground pixels. Maximally stable external regions feature extraction strategies used for extracting features from ROI of a frame. Finally, object detection and tracking step is implemented with RNN.
<h3>3.8 Video anomaly detection using GE based promotion method</h3>
The entire process of GE based deep learning methods consist of two steps: training and testing. The promotion method is implemented during the testing stage of detection. One of the main reason for implementing GE-based method is to set a single threshold value to all the video frames subjected to detect anomalies. This is done by considering maximum value of GE for detecting anomalies in frames using max pooling and mean filter operation. During training, a generator is used to produce normal patterns. To learn information of the whole scene, frame level GE is considered. After the training process a bidirectional LSTM is used for generating GE maps. Then the block level process is performed with max pooling and mean filtering. In block level process GE saliency of normal and abnormal frames are compared. The saliency is computed: $saliency = \frac{L_{abnormal} - L_{normal}}{L_{normal}}$. The maximum value of block level saliency is used for detecting anomalies in frames: $L_B(t) = max\{L_{B1}(t), L_{B2}(t),...,L_{Bk}(t)\}$, where $k$ is the number of blocks in a frames and $L_{Bk}(t)$ is the $k^{th}$ block level GE: $L_{Bk}(t) = \frac{1}{h \times w}\sum_{i = 1}^h\sum_{j = 1}^wB_{i, j, k}$. Finally, an anomaly score is calculated to find frames that contain  anomalous behaviour. 
<h2>4. Research challenges</h2>
- Unavailability of better dataset: imbalanced distribution of normal and abnormal behaviour data.
- Environmental challenges: background clutter, occlusions and illuminations.
- Time and space complexity: developing a simple, efficient and accurate system is still a challenging problem.
- Dynamic behaviour in anomalies: a single technique is not capable of detecting all types of anomalies.
- Atmospheric turbulences: light, smoke and fog which blur the images in the video.