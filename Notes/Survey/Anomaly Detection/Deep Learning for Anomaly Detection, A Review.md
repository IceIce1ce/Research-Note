<h2>1. Abstract</h2>
This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of methods. Review their key intuitions, objective functions, underlying assumptions, advantages and disadvantages and discuss how they address the aforementioned challenges.
<h2>2. Anomaly detection: problem complexities and challenges</h2>
<h3>2.1 Major problem complexities</h3>
Unknownness, heterogeneous anomaly classes, rarity and class imbalance, diverse types of anomaly.
<h3>2.2 Main challenges tackled by deep anomaly detection</h3>
Low anomaly detection recall rate, anomaly detection in high-dimensional and/or not-independent data, data-efficient learning of normality/abnormality, noise-resilient anomaly detection, detection of complex anomalies and anomaly explanation.
<h2>3. Deep learning for feature extraction</h2>
$z = \phi(x; \Theta)$, where $\phi: \mathcal{X} \rightarrow \mathcal{Z}$ is a deep neural network-based feature mapping function, with $\mathcal{X} \in \mathbb{R}^D, \mathcal{Z} \in \mathbb{R}^K$ and normally $D >> K$. An anomaly scoring method f is then applied onto the new space to calculate anomaly scores. Advantages: 1) a large number of SOTA deep models and off-the-shelf anomaly detectors are readily available, 2) deep feature extraction offers more powerful dimensionality reduction than popular linear methods and 3) it is easy to implement given the public availability of the deep models and detection methods. Disadvantages: 1) the fully disjointed feature extraction and anomaly scoring often lead to suboptimal anomaly scores, 2) pre-trained deep models are typically limited to specific types of data.
<h2>4. Learning feature representations of normality</h2>
<h3>4.1 Generic normality feature learning</h3>
This category of methods learns the representations of data instances by optimizing a generic feature objective function that is not primarily designed for anomaly detection. ${\Theta^*, W^* = \mathrm{argmin}_{\Theta, W}\sum_{x \in \mathcal{X}}l(\psi(\phi(x; \Theta); W))}, s_x = f(x, \phi_{\Theta^*}, \psi_{W^*})$, where $\phi$ maps the original data onto representation space $\mathcal{Z}$, $\psi$ parameterized by W is a surrogate learning task that operates on $\mathcal{Z}$ space, $l$ is a loss function and $f$ is a scoring function that utilizes $\phi$ and $\psi$ to calculate anomaly score $s$.
<h4>4.1.1 Autoencoders</h4>
Advantages: 1) the idea of AEs is straightforward and generic to different types of data, 2) different types of powerful AE variants can be leveraged to perform anomaly detection. Disadvantages: 1) the learned feature representations can be biased by infrequent regularities and the presence of outliers or anomalies in the training data, 2) the objective function of data reconstruction is designed for dimension reduction of data compression rather than anomaly detection.
<h4>4.1.2 Generative adversarial networks</h4>
Advantages: 1) GANs have demonstrated superior capability in generating realistic instances, especially on image data, empowering the detection of abnormal instances that are poorly reconstructed from the latent space, 2) a large number of existing GAN-based models and theories may be adapted for anomaly detection. Disadvantages: 1) the training of GANs can suffer from multiple problems such as failure to converge and model collapse, 2) the generator network can be misled and generates data instances out of manifold of normal instances, especially when true distribution of given dataset is complex or training data contain unexpected outliers, 3) GANs-based anomaly scores can be suboptimal.
<h4>4.1.3 Predictability modeling</h4>
It learn feature representations by predicting current data instances using representations of previous instances within a temporal window as context. Advantages: 1) a number of sequence learning techniques can be adapted and incorporated into this approach, 2) this approach enables the learning of different types of temporal and spatial dependencies. Disadvantages: 1) this approach is limited to anomaly detection in sequence data, 2) the sequential predictions can be computationally expensive, 3) the learned representations may suboptimal for anomaly detection. 
<h4>4.1.4 Self-supervised classification</h4>
This approach learns representations of normality by building self-supervised classification models and identifies instances that are inconsistent to the classification models as anomalies. Advantages: 1) they work well in both unsupervised and semi-supervised settings, 2) anomaly scoring is grounded by some intrinsic properties of gradient magnitude and its updating. Disadvantages: 1) feature transformation operations are often data dependent, 2) although the classification model is trained in an end-to-end manner, the consistency-based anomaly scores are derived upon the classification scores rather than an integrated module in the optimization and thus they may be suboptimal.
<h3>4.2 Anomaly measure-dependent feature learning</h3>
It aims at learning feature representations that are optimized for one particular existing anomaly measure. Formally, the framework for this group of methods can be represented as: ${\Theta^*, W^* = \mathrm{argmin}_{\Theta, W}\sum_{x \in \mathcal{X}}l(f(\phi(x; \Theta); W))}, s_x = f(\phi(x; \Theta^*), W^*)$, where $f$ is an existing anomaly scoring measure operating on the representation space.
<h4>4.2.1 Distance-based measure</h4>
It aims to learn feature representations that are specifically optimized for a specific type of distance-based anomaly measures. However, it fails to work effectively in high-dimensional data due to the curse of dimensionality. Advantages: 1) distance-based anomalies are straightforward and well defined with rich theoretical supports in the literature. Thus, deep distance-based anomaly detection methods can be well grounded due to the strong foundation built in previous relevant work, 2) they work in low-dimensional representation spaces and can effectively deal with high-dimensional data that traditional distance-based anomaly measures fail, 3) they are able to learn representations specifically tailored for themselves. Disadvantages: 1) the extensive computation involved in most of distance-based anomaly measures may be an obstacle to incorporate distance-based anomaly measures into representation learning process, 3) their capabilities may be limited by the inherent weaknesses of the distance-based anomaly measures.
<h4>4.2.2 One-class classification-based measure</h4>
It aims to learn feature representations customized to subsequent one-class classification-based anomaly detection. Advantages: 1) the one-class classification-based anomalies are well studied in the literature and provides a strong foundation of deep one-class classification-based methods, 2) the representation learning and one-class classification models can be unified to learn tailored and more optimal representations, 3) they free the users from manually choosing suitable kernel functions in traditional one-class models. Disadvantages: 1) the one-class models may work ineffectively in datasets with complex distributions within the normal class, 2) the detection performance is dependent on the one-class classification-based anomaly measures.
<h4>4.2.3 Clustering-based measure</h4>
It aims at learning representations so that anomalies are clearly deviated from the clusters in the newly learned representation space. Advantages: 1) a number of deep clustering methods and theories can be utilized to support the effectiveness and theoretical foundation of anomaly detection, 2) compared to traditional clustering-based methods, deep clustering-based methods learn specifically optimized representations that help spot the anomalies easier than on the original data, especially when dealing with intricate datasets. Disadvantages: 1) the performance of anomaly detection is heavily dependent on clustering results, 2) the clustering process may be biased by contaminated anomalies in the training data, which in turn leads to less effective representations.
<h2>5. End-to-end anomaly score learning</h2>
It aims at learning scalar anomaly scores in an end-to-end fashion. Compared to anomaly measure-dependent feature learning, the anomaly scoring in this type of approach is not dependent on existing anomaly measures, it has a neural network that directly learns the anomaly scores. Formally, this approach aims at learning an end-to-end anomaly score learning network: $\tau(\cdot; \Theta): \mathcal{X} \rightarrow \mathbb{R}$. The underlying framework can be represented as: $\Theta^* = \mathrm{argmin}_\Theta \sum_{x \in \mathcal{X}}l(\tau(x; \Theta)), s_x = \tau(x, \Theta^*)$. 
<h3>5.1 Ranking models</h3>
It aims to directly learn a ranking model such that data instances can be sorted based on observable ordinal variable associated with the absolute/relative ordering relation of the abnormality. The anomaly scoring neural network is driven by the observable ordinal variable. Advantages: 1) the anomaly scores can be optimized directly with adapted loss functions, 2) they are generally free from the definitions of anomalies by imposing a weak assumption of the ordinal order between anomaly and normal instances, 2) this approach may build upon well-established ranking techniques and theories from areas like learning to rank. Disadvantages: 1) at least some form of labeled anomalies are required in these methods which may not be applicable to applications where such labeled anomalies are not available, 2) since the models are exclusively fitted to detect few labeled anomalies, they may not be able to generalize to unseen anomalies that exhibit different abnormal features to the labeled anomalies.
<h3>5.2 Prior-driven models</h3>
This approach uses a prior distribution to encode and drive the anomaly score learning. Since the anomaly scores are learned in an end-to-end manner, the prior may be imposed on either the internal module or the learning output of the score learning function $\tau$. Advantages: 1) the anomaly scores can be directly optimized w.r.t a given prior, 2) it provides a flexible framework for incorporating different prior distributions into the anomaly score learning. Different bayesian deep learning techniques may be adapted for anomaly detection, 3) the prior can also result in more interpretable anomaly scores than other methods. Disadvantages: 1) it is difficult, if not impossible, to design a universally effective prior for different anomaly detection application scenarios, 2) the models may not work less effectively if the prior does not fit the underlying distribution well.
<h3>5.3 Softmax likelihood models</h3>
This approach aims at learning anomaly scores by maximizing the likelihood of events in the training data. Since anomaly and normal instances correspond to rare and frequent patterns, from the probabilistic perspective, normal instances are presumed to be high-probability events whereas anomalies are prone to be low-probability events. Therefore, the negative of the event likelihood can be naturally defined as anomaly score. Advantages: 1) different types of interactions can be incorporated into the anomaly score learning process, 2) the anomaly scores are faithfully optimized w.r.t the specific abnormal interactions to capture. Disadvantages: 1) the computation of the interactions can be very costly when the number of features/elements in each data instance is large, 2) the anomaly score learning is heavily dependent on the quality of the generation of negative samples. 
<h3>5.4 End-to-end one-class classification</h3>
This category of methods aims to train a one-class classifier that learns to discriminate whether a given instance is normal or not in an end-to-end manner. This approach does not rely on any existing one-class classification measures such as one-class SVM or SVDD. This approach emerges mainly due to the marriage of GANs and concept of one-class classification. The key idea is to learn a one-class discriminator of the normal instances so that it well discriminates those instances from adversarially generated pseudo anomalies. Advantages: 1) its anomaly classification model is adversarially optimized in an end-to-end fashion, 2) it can be developed an d supported by the affluent techniques and theories of adversarial learning and one-class classification. Disadvantages: 1) it is difficult to guarantee that the generated reference instances well resemble the unknown anomalies, 2) the instability of GANs may lead to generated instances with diverse quality and consequently unstable anomaly classification performance, 3) its applications are limited to semi-supervised anomaly detection scenarios. 
<h2>6. Future opportunities</h2>
Exploring anomaly-supervisory signals, deep weakly supervised anomaly detection, large-scale normality learning, deep detection of complex anomalies, interpretable and actionable deep anomaly detection and novel applications and settings.