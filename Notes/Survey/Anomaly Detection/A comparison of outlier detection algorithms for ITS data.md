<h2>1. Abstract</h2>
Three typical outlier algorithms: statistics-based, distance-based ad density-based local outlier approach are described with the characteristics and the time complexity of the algorithms. A comparison among three algorithms is made through application to ITS. Two traffic datasets with different dimensions have been used in experiments, one is travel time data and the other is traffic flow data. In addition, some artificial generated outliers are introduced into traffic flow data to see how well the different algorithms detect them. Three strategies-based on ensemble learning, partition and average LOF have been proposed to develop a better outlier recognizer.
<h2>2. Introduction</h2>
There could be two types of outliers: a) outliers caused by measurement error or equipment failure and b) outliers reflecting ground truth. 
<h2>3. Approaches of outlier detection</h2>
Outlier mining actually consists of two sub-problems: firstly, to define what kind of data is deemed to be exceptional in the given dataset and secondly, to find an efficient algorithm to obtain such data.
<h3>3.1 Statistics-based outlier detection approach</h3>
The main idea of this approach assumes a distribution or probability model for given dataset and then identifies outliers with respect to model using a discordancy test. A statistical discordancy test examines two hypotheses, a working hypothesis and an alternative hypothesis. The working hypothesis, $H: x_i \in F, i = 1, 2,...,n$ assumes the entire dataset of n comes from an initial same distribution model F while the alternative hypothesis $\overline{H}: x_i \in G, i = 1,2,...,n$. Such kind of algorithm is simple and its time complexity is $O(n)$. However, the result is very much dependent on model F chosen since $x_i$ may be an outlier under one model and a perfect value under another. Another major drawback is that it is only appropriate for one-dimensional data but not multi-dimensional data. Multivariate outlier identification procedures often depend on a Mahalanobis distance or a related statistic, Hotelling's $T^2$-statistic. 
<h3>3.2 Distance-based outlier detection approach</h3>
An object x in dataset X is defined as an outlier with parameters p and d, described as DB(p, d) if a fraction of p of the objects in X lie at a distance greater than d from x. Suppose M = n*(1 - p) where n is the number of data, outlier detection is a process that determine whether the number of points which are at a distance less than d from x is more than M. If so, x is not an outlier and otherwise. The algorithm runs in O(k * n$^2$) with respect to the worst time complexity, where k is the number of dimensions. However, this approach is sensitive to parameter p and d and results are instable for this reason. In addition, it has to calculate the distance between all samples in k dimension dataset, the efficiency is low for great dataset in high dimensional space. 
<h3>3.3 Density-based outlier detection approach</h3>
Given a positive integer k, define the k-distance of x, denoted as k-distance(x) is defined as the distance d(x, o) between object x and object o $\in$ D such that: 
1) For at least exists k object o' $\in$ D \ {x}, it holds that d(x, o') $\leq$ d(x, o) and
2) For at most exists k - 1 objects o' $\in$ D \ {x}, it holds that d(x, o') < d(x, o).
Define the k-distance neighborhood of x as: N$_k$(x) = {q $\in$ D \ {x} | d(x, q) $\leq$ k - distance(x)}. The local reachability distance of object x with respect to object o is defined as: reach-disp$_k$(x, o) = max{k - distance(o), d(x, o)}. The local reachability density of x is: lrd$_k$(x) = $\frac{1}{\frac{\sum_{o \in N_k(x)}reach\_disp_k(o)}{|N_k(x)|}}$, where $|N_k(x)|$ is the cardinality of $N_k(x)$. The local outlier factor of x is: $lof_k(x) = \frac{\sum_{o \in N_k(x)}\frac{lrd_k(o)}{lrd_k(x)}}{|N_k(x)|}$.