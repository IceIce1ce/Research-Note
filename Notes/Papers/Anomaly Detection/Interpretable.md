<h2>1. Introduction</h2>
Demonstrate that the scene graphs obtained by monitoring the object interactions provide an interpretation for the context of the anomaly while performing competitively with respect to the recent state-of-the-art approaches. Moreover, the proposed interpretable method enables cross-domain adaptability.
<h2>2. Proposed technique</h2>
<h3>2.1 Overall structure</h3>
The proposed approach consists of two branches of monitoring, namely global and local object monitoring, followed by sequential anomaly detection. The global object monitoring branch exclusively observes the interactions between different objects in the scene and generates a scene graph, whereas the local object monitoring branch monitors each person in the video independently. The sequential anomaly detection block, in the end, monitors the statistics of the two object monitoring blocks to detect anomalous events in a quick and reliable manner.
<h3>2.2 Global object monitoring</h3>
- Interaction detection: the proposed method first detects the bounding boxes in each frame and then processes them in pairs. A CNN is used to extract the individual appearance features from each bounding box separately (Individual CNN). In parallel, another CNN processes the union of bounding boxes to extract the joint appearance features (Joint CNN). The joint and individual appearance features are concatenated and passed through a MLP to obtain a score vector $v_1$ for each predicate class present in VRD dataset as well as the no-predicate class. Similarly, the individual appearance features are processed by two MLPs to obtain two ($K + 1$)-dimensional score vectors $v_2$ and $v_3$, where $K$ denotes the number of predicate classes. Another score vector $v_4$ is provided by the Semantic Module which takes the object labels from the Individual CNN and outputs the score $\log(q_i/q_\oslash)$ for each predicate class i using the empirical probabilities $\{q_i,...,q_K,q_\oslash\}$ of the predicate classes for the considered subject-object pair from the annotated training data. The final step in interaction detection is summing the four score vectors and applying the softmax function to compute the predicate class probabilities. In training with the VRD dataset for each image with $M$ bounding boxes, considering all $N = M(M - 1)$ subject-object pairs the following loss is computed and backpropagated through the MLPs that produce $v_1, v_2, v_3$ as well as Joint CNN: $L_d = \sum_{i = 1}^N\frac{1}{N}[max\{0, \alpha_1 - m^s_1(i)\} + max\{0, \alpha_1 - m^o_1(i)\}] - \lambda\log p_{i^*}$, where $\alpha_1$ is the margin threshold and $i^* \in \{1,...,K+1\}$ denotes the actual predicate class.
- Interaction monitoring: For each detected interaction $i$ in each frame $t$, the (subject, predicate, object) triplet with the highest probability predicate class is monitored for possible anomaly evidence. First, the word triplet is mapped to numerical vectors using Word2Vec. Then, the average of the three embeddings, $a^i_t$ is input to an MLP for metric learning. To that end, use another contrastive loss which minimizes the Euclidean distance between the nominal embeddings and maximizes the distance between nominal and anomalous embeddings. Denoting the metric learning embedding with $g(\cdot)$, the objective is to minimize $||g(a^i_t) - g(p)||$ and maximize $||g(a^i_t) - g(n)||$ where $a^i_t, p, n$ represent the semantic embedding vector of the anchor, positive, and negative instances. While the positive instance (i.e., interaction triplet) $p$ is randomly selected from the nominal training set, the negative instance is randomly sampled from an artificially generated set of anomalous interaction triplets, e.g., person hits person. During training, only nominal instances are used as anchor. $L_m = max\{0, \alpha_2 + ||g(a) - g(p)|| - ||g(a) - g(n)||\} + \mu||g(a)||^2$, where $\mu$ helps combine the contrastive loss with the L2- regularizer, which ensures small embeddings for nominal (positive) instances during training. $GM_{stat}(t) = max_ig(a^i_t)$.
<h3>2.3 Local object monitoring</h3>
In each frame at time $t$, pose $i$ is represented by a set of joint locations in image coordinates $\theta^i_t = (x^i_t, y^i_t)_{i = 1,...,J}$, where $J$ is the number of pose joints. Propose the local monitoring branch where both the encoder and decoder networks are divided into two GRU branches. While one row of GRU processes global features, the other processes local feature. $LM_{stat}(t) = max_i||\hat{\theta}^i_t - \theta^i_t||$.
<h3>2.4 Sequential anomaly detection</h3>
The global and local monitoring statistics from each frame $t$ are combined $z_t = [GM_{stat}(t), LM_{stat}(t)]$ and sequentially monitored for possible anomalies. In order to statistically monitor $z_t$ in a sequential manner, measure its Euclidean distance to the nominal training se $Z$. In particular, the distance $d_t$ to $k$NN in $Z$ is computed and compared to a nominal baseline $d_\alpha$ to quantify any anomaly evidence in frame $t$. The nominal baseline $d_\alpha$ is obtained in training as $(1 - \alpha)$ percentile of $k$NN distances of training instances with respect to each other, where $\alpha = 0.05$. During testing, for each vector $z_t$ at time $t$, the proposed algorithm computes the instantaneous frame-level anomaly evidence $\delta_t$ as: $\delta_t = \log d^2_t - \log d^2_\alpha$. Finally, update a sequential decision statistic $s_t$ as: $s_t = max\{s_{t - 1} + \delta_t, 0\}, s_0 = 0$. There is an anomaly when the decision statistic t exceeds the threshold $h$.
<h2>3. Datasets</h2>
Avenue and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.