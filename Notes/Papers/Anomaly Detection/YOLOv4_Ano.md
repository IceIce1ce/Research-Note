<h2>1. Abstract</h2>
The proposed framework consists of three hierarchical steps, including efficient and accurate object detection based on the stateof-the-art YOLOv4 method, object tracking based on Kalman filter coupled with the Hungarian algorithm for association, and accident detection by trajectory conflict analysis. A new cost function is applied for object association to accommodate for occlusion, overlapping objects, and shape changes in the object tracking step. The object trajectories are analyzed in terms of velocity, angle, and distance in order to detect different types of trajectory conflicts including vehicle-to-vehicle, vehicle-to-pedestrian, and vehicle-to-bicycle.
<h2>2. Methodology</h2>
<h3>2.1 Road-user detection</h3>
Apply YOLOv4 model pre-trained on MS COCO dataset.
<h3>2.2 Road-user tracking</h3>
Sort is used as main tracking model. Considering two adjacent video frames $t$ and $t$ + 1, two sets of objects detected at each frame are: $O^t = \{o^t_1,...,o^t_n\}, O^{t + 1} = \{o^{t + 1}_t,...,O^{t + 1}_m\}$. Every object $o_i$ in set $O^t$ is paired with an object $o_j$ in set $O^{t + 1}$ that can minimize cost function $C(o_i, o_j)$. The index $i \in [N] = 1,...,N$ denotes the objects detected at previous frame and index $j \in [M] = 1,...,M$ represents the new objects detected at current frame. The appearance distance is calculated based on the histogram correlation between and object $o_i$ and a detection $o_j$: $C^A_{i, j} = 1 - \frac{\sum_b(H_b(o_i) - \overline{H}(o_i))(H_b(o_j) - \overline{H}(o_j))}{\sqrt{\sum_b(H_b(o_i) - \overline{H}(o_i))^2\sum_b(H_b(o_j) - \overline{H}(o_j))^2}}$, where $C^A_{i, j}$ is a value between 0 and 1, $b$ is the bin index, $H_b$ is the histogram of an object in RGB and $\overline{H}(o_k) = \frac{1}{B}\sum_b H_b(o_k)$, in which $B$ is the total number of bins in the histogram of an object $o_k$. The size dissimilarity is calculated: $C^S_{i, j} = \frac{1}{2}(\frac{|h_i - h_j|}{h_i + h_j} + \frac{|w_i - w_j|}{w_i + w_j})$. The position dissimilarity is computed: $C^P_{i, j} = \frac{1}{2}(\frac{|x_i - x_j|}{x_i + x_j} + \frac{|y_i - y_j|}{y_i + y_j})$, where value of $C^P_{i, j}$ is between 0 and 1. Also use IOU value to calculate Jaccard distance: $C^K_{i, j} = 1 - \frac{Box(o_i) \cap Box(o_j)}{Box(o_i) \cup Box(o_j)}$, where $Box(o_k)$ denotes set of pixels contained in bounding box of object $k$. The overall dissimilarity value is calculated as: $C_{i, j} = w_aC^A_{i, j} + w_sC^C_{i, j} + w_pC^P_{i, j} + w_aC^A_{i, j} + w_kC^K_{i, j}$, in which $w_a, w_s, w_p$ and $w_k$ define the contribution of each dissimilarity value in total cost function. If the dissimilarity between a matched detection and track is above a certain threshold, the detected object is initiated as a new track.
<h3>2.3 Accident detection</h3>
First, the Euclidean distances among all object pairs are calculated in order to identify the objects that are closer than a threshold to each other. A predefined number $f$ of consecutive video frames are used to estimate speed of each road-user individually. The average bounding box centers associated to each track at first half and second half of $f$ frames are computed. Two average points $p$ and $q$ are transformed to real-world coordinates using inverse of homography matrix $H^{-1}$, which is calculated during camera calibration by selecting a number of points on the frame and their corresponding locations on the Google Maps. The distance in kilometers can then be calculated by applying the haversine formula as follows: $h = \sin^2(\frac{\phi_q - \phi_p}{2}) + \cos\phi_p \cdot \cos\phi_q \cdot \sin^2(\frac{\lambda_q - \lambda_p}{2}), d_h(p, q) = 2r\mathrm{arcsin}(\sqrt{h})$, where $\phi_p$ and $\phi_q$ are latitudes, $\lambda_p$ and $\lambda_q$ are longtitudes  of first and second averaged points $p$ and $q$, $h$ is the haversine of central angle between two points, $r \approx 6371$ kilometers is the radius of earth. The speed $s$ of tracked vehicle can be estimated as: $S = \frac{d_h(p, q) \times 3600 \times fps}{f}$. Another factor is the angle of collision. The bounding box centers of each road-user are extracted at two points: (i) when they are first observed and (ii) at the time of conflict with another road-user. Then the approaching angle of the a pair of road-users $a$ and $b$ is calculated as: $m_a = \frac{(y^t_a - y^{t'}_a)}{(x^t_a - x^{t'}_a)}, m_b = \frac{(y^t_b - y^{t''}_b)}{(x^t_b - x^{t''}_b)}, \theta = arctan(\frac{m_a - m_b}{1 + m_am_b})$, where $\theta$ denotes the estimated approaching angle, $m_a$ and $m_b$ are general moving slopes of road-users $a$ and $b$ with respect to origin of video frame $x^t_a, y^t_a, x^t_b, y^t_b$ represent center coordinates of road-users $a$ and $b$ at current frame, $x^{t'}_a$ and $y^{t'}_a$ are center coordinates of object $a$ when first observed, $x^{t''}_b$ and $y^{t''}_b$ are center coordinates of object $b$ when first observed. If the bounding boxes of the object pair overlap each other or are closer than a threshold the two objects are considered to be close. Trajectory conflicts involve near-accident and accident occurrences and include three types, namely, vehicle-to-vehicle (V2V), vehicleto-pedestrian (V2P), and vehicle-to-bicycle (V2B).
<h2>3. Datasets</h2>
29 short videos from YouTube that contain 24 V2V, 2 V2B, 3 V2P trajectory conflict cases. The dataset includes day-time and night-time. Resolution: 1280 $\times$ 720 pixels with FPS 30.
<h2>4. Metrics</h2>
Detection and false alarm rates.
<h2>5. Code</h2>
https://github.com/hadign20/AccidentDetection.