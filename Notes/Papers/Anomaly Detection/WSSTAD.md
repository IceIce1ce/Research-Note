<h2>1. Abstract</h2>
Given an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e., a sequence of bounding boxes at consecutive times) that encloses the abnormal event, with only coarse videolevel annotations as supervision during training. Propose a dualbranch network which takes as input the proposals with multi-granularities in both spatial-temporal domains. Each branch employs a relationship reasoning module to capture the correlation between tubes/videolets. Mutually-guided Progressive Refinement framework is set up to employ dual-path mutual guidance in a recurrent manner, iteratively sharing auxiliary supervision information across branches. Furthermore, contribute two datasets, i.e., ST-UCF-Crime and STRA, consisting of videos containing spatio-temporal abnormal annotations to serve as the benchmarks for WSSTAD.
<h2>2. Methodology</h2>
<h3>2.1 Spatio-temporal instance generation</h3>
Design two kinds of tube-level instance from different gradations. Unary tube encloses an individual object and multivariate tube contains multiple intersecting objects.
<h3>2.2 Relationship modeling prediction</h3>
Design a dualbranch network architecture, which contains one tube branch that employs tube-level instances to capture spatial cues, and another temporal branch that leverages videolet-level instances for exploiting temporal correlation. In each branch, instance features $\mathcal{F}$ are first extracted by a C3D model. The relationship modeling based self-attentive representation $\hat{\mathcal{F}} = (\hat{\mathcal{F_1}},...,\hat{\mathcal{F_n}})$ is computed via residual connection: $\hat{\mathcal{F}} = g_{ma}(\mathcal{F}, \mathcal{F}, \mathcal{F}) + \mathcal{F}$. The obtained $\hat{\mathcal{F_i}}$ is then fed into 3-layer FC layers to predict abnormal score.
<h3>2.3 Mutually-guided progressive refinement</h3>
- MIL: use $\mathcal{T}^{a, r}, \mathcal{T}^{a, g}$ to denote region-level based tube instances and image-level based tube instances in anomalous video, $\mathcal{T}^{n, r}$ is region-level based tube instances in normal video. Furthermore, use $p_t, p_v$ to represent scores predicted by tube and temporal branch.
- Dual-path mutual guidance: a novel mutually-guided ranking loss is designed to leverage the feedback of each branch to serve as a guidance for its counterpart. Specifically, feed max instance of tube branch into temporal branch and outputs an abnormal score, which is treated as a confidence weight of the ranking loss that guides the training of the tube branch: $\mathcal{L}^{tube}_{MG-Rank} = max(0, p_v(\mathcal{T}^{a, g}) \times (1 - p_t(\mathcal{T}^{a, r}_m + p_t(\mathcal{T}^{n, r}_m)))$, where $\mathcal{T}^{a, r}_m, \mathcal{T}^{a, g}_m, \mathcal{T}^{n, r}_m$ denotes max instance obtained in tube branch. Similarly, temporal branch is optimized by: $\mathcal{L}^{tem}_{MG-Rank} = max(0, p_t(\mathcal{V}^a_m) \times (1 - p_v(\mathcal{V}^a_m) + p_v(\mathcal{V}^n_m)))$, where $\mathcal{V}^a_m, \mathcal{V}^n_m$ denotes max instance in temporal branch. In each training iteration, first freeze the tube branch and provide loss weights to optimize the temporal branch. Then switch to training the tube branch and obtain weights from the temporal branch. This alternate optimization procedure is repeated iteratively during training: $\mathcal{L}_{MG-Rank} = \psi \times \mathcal{L}^{tube}_{MG-Rank} + (1 - \psi) \times \mathcal{L}^{tem}_{MG-Rank}$, where $\psi$ is a binary variable indicating selection of training branch.
- CE loss: $\mathcal{L}_{CE} = -[\log(p_t(\mathcal{T}^{a, r}_m)) + \log(1 - p_t(\mathcal{T}^{n, r}_m))] - [\log(p_v(\mathcal{V}^a_m)) + \log(1 - p_v(\mathcal{V}^n_m))]$. The finall loss is defined: $\mathcal{L} = \mathcal{L}_{MG-Rank} + \mathcal{L}_{CE}$.
<h3>2.4 Inference</h3>
$\mathcal{T}_{pred} = argmax_{\mathcal{T}_i}\frac{p_t(\mathcal{T}^r_i) + p_v(\mathcal{T}^g_i)}{2}$.
<h2>3. Datasets</h2>
ST-UCF-Crime and STRA.
<h2>4. Metrics</h2>
VAUC and MIoU.