<h2>1. Abstract</h2>
Develop a Deep Q Learning Network (DQN) to localize anomalies in videos by enabling the agent to learn how to detect and recognize the abnormalities in videos. Our idea is inspired by multiple instance learning (MIL) techniques based on common share features with reinforcement learning. We consider normal and abnormal videos as bags and the selection of videos clips as actions. In our DQN architecture we will design a fully connected layer, which compute probability for each video segment in both positive(anomalous) and negative(normal) bags indicating how likely a clip is containing an anomaly.
<h2>2. Proposed approach</h2>
Handle the anomaly detection as Markov Decision Process and introduce a deep reinforcement learning approach to train the evaluation network. Present a deep model to evaluate the situation of video segment $X_i$ and calculate its anomaly weight $Q_i$: $Q_i = C(S_t, X_i), Q^*_i = C_1(S_t, X_i)$, where $S_t$ represent the state at time $t$ regrouping all segments of normal/abnormal videos. To select the most representative segment from the video, we can employ two different strategies. One is to immediately score each segment and then determine the most representative segment from each video. The other is to drop the worst segment progressively, and the last remaining segment is considered as the most representative one. Adopt Deep Q network architecture to determine the most representative clip in a set of a video segment. We designate a set of segments in a video as state $S_t$ which is defined as: $\forall X \in V_{n , a,} S_t = X_1,...,X_n$. The action of choosing to drop the worst segments from both normal/abnormal video states leads to a state $S_{t + 1}$. . The following formula describes the segment selection process within state $S_t$: $(X_i | S_{t + 1}) = 0 if X_{i^*} = \mathrm{argmin}_X Q(S_t, X_i)$ and 1 otherwise. The policy is defined as $\pi$ where: $\pi = \mathrm{argmax}_{a_i}Q_i$. Therefore, $Q_i$ can be rewritten as: $Q(S_t, a_i) = R(s_t, a_i) + \gamma \mathrm{max}_a Q(S_{t + 1}, \mathrm{argmax}_a Q(S_{t + 1}, a)), R(S_t, a_i) = Q^*(S_t, a_i)$. By default, consider a state $S_t$ as a set of 32 segments per videos, our Q-network produces a Q-value for each segment in the state. But, In order to avoid a false positive anomaly segment, we proposed to add a new special state $S_{ts}$, which include only two segments having the higher Q-value in the normal and abnormal state. $S_{ts} = {\mathrm{max}_{X_i \in V_n}(X_i), \mathrm{max}_{X_i \in V_a}(X_i)}$. To avoid to correlation issue between consecutive states, we decide to use the experience replay technique , which it consists of storing agents experience $(S_t; X_t; R_t; S_{t + 1})$ at each time step into a dataset D, where $S_{t + 1}$ is the state at the next time step $t$ + 1. For updating Q-values, it uses stochastic minibatch updates with uniformly random sampling from experience replay memory. For a better final policy quality, the network is trained with a target Q-network $\theta$ to obtain consistent Q-learning targets by fixing weight parameters used in Q-learning target and updating them periodically. 
<h2>3. Datasets</h2>
UCF-Crime.
<h2>4. Metrics</h2>
Accuracy.