<h2>1. Abstract</h2>
Anomaly detection with weakly supervised video-level labels is typically formulated as a multiple instance learning problem --> aim to identify snippets containing abnormal events with each video represented as a bag of video snippets. Rare abnormal snippets in the abnormal videos is largely biased by the dominant negative instances, especially when abnormal events are subtle anomalies that exhibit only small differences compared with normal events. Introduce RTFM which trains a feature magnitude learning function to effectively recognise positive instances. RTFM also adapts dilated convolutions and self-attention mechanisms to capture long-and sort-range temporal dependencies to learn feature magnitude more faithfully.
<h2>2. Introduction</h2>
Weakly-supervised uses training samples with video-level label annotations of normal and abnormal. Setup targets a better anomaly classification accuracy at the expense of a relatively small human annotation effort, compared with one-class classifier approaches. Major challenges: how to identify anomalous snippets from a whole video labelled as abnormal. Two reasons: 1) majority of snippets from an abnormal video consist of normal events, which can overwhelm training process and challenge fitting of few abnormal snippets and 2) abnormal snippets may not be sufficiently different from normal ones. MIL problems: 1) top anomaly score in an abnormal video may not be from an abnormal snippet, 2) normal snippets randomly selected from normal videos may be easy to fit, 3) if video has more than one abnormal snippet, miss the chance of having a more effective training process containing more abnormal snippets per video and 4) use of classification score provides a weak training signal that does not enable a good separation between normal and abnormal snippets. In RTFM, rely on temporal feature magnitude of video snippets, where features with low magnitude represent normal snippets and otherwise. It enforces large margins between top k snippet features with largest magnitudes from abnormal and normal videos which guarantees to maximally separate abnormal and normal video representations. These top k snippet features from normal and abnormal videos are then selected to train a snippet classifier. Combine learning of long and short-range temporal dependencies with a pyramid of dilated convolutions (PDC) and a temporal self-attention module (TSA).
<h2>3. Related work</h2>
<h3>3.1 Unsupervised anomaly detection</h3>
Traditional anomaly detection methods assume availability of normal training data only and address problem with one-class classification using handcrafted features. Recent approaches use features from pre-trained deep neural networks. Others apply constraints on the latent space of normal manifold to learn compact normality representations. Some approaches depend on data reconstruction using generative models to learn representations of normal samples by minimising reconstruction error.
<h3>3.2 Weakly supervised anomaly detection</h3>
Use cheaper video-level annotations, based on MIL framework. However, fail to leverage abnormal video labels as they can be affected by label noise in the positive bag caused by a normal snippet mistakenly selected as top abnormal event in an anomaly video.
<h2>4. RTFM</h2>
Given a set of weakly-labelled training videos $\mathcal{D}$ = {(F$_{i, y_i})$}$^{|\mathcal{D}|}_{i = 1}$, where F $\in \mathcal{F} \subset \mathbb{R}^{T \times D}$ are pre-computed features (I3D or C3D) of dimension $D$ from $T$ video snippets and $y \in \mathcal{Y}$ = {0, 1} denotes the video-level annotation ($y_i$ = 0 if F$_i$ is a normal video and $y_i$ = 1 otherwise). The model used by RTFM is denoted by $r_{\theta, \phi}$(F) = $f_\phi(s_\theta$(F)) and returns a T-dimensional feature [0, 1]$^T$ representing classification of $T$ video snippets into abnormal or normal. Loss: min$_{\theta, \phi}\sum_{i, j = 1}^{|\mathcal{D}|}l_s(s_\theta(\mathrm{F}_i), (s_\theta(\mathcal{F}_j)), y_i, y_j) + l_f(f_\theta(s_\theta(\mathrm{F}_i)), y_i)$ (1), where $s_\theta: \mathcal{F} \rightarrow \mathcal{X}$ is temporal feature extractor (with $\mathcal{X} \subset \mathrm{R}^{T \times D}$), $f_\phi: \mathcal{X} \rightarrow [0, 1]^T$ is the snippet classifier, $l_s(\cdot)$ denotes a loss function that maximises the separability between top-k snippet features from normal and abnormal videos and $l_f(\cdot)$ is a loss function to train snippet classifier $f_\phi(\cdot)$ also using top-k snippet features from normal and abnormal videos. 
<h3>4.1 Theoretical motivation of RTFM</h3>
Top-k MIL extends MIL to an environment where positive bags contain a minimum number of positive samples and negative bags also contain a positive samples and it assumes that a classifier can separate positive and negative samples. A temporal feature extracted from a video is denoted by X = $s_\theta$(F) in (1), where snippet features are represented by rows x$_t$ of X. An abnormal snippet is denoted by x$^+$ ~ $P^+_x$(x) and a normal snippet x$^-$ ~ $P^-_x$(x). An abnormal video X$^+$ contains $\mu$ snippets drawn from $P^+_x$(x) and ($T - \mu$) drawn from $P^-_x$(x) and a normal video X$^-$ has all $T$ snippets sampled from $P^-_x$(x). To learn a function that can classify videos and snippets as normal or abnormal, rely on an optimisation based on mean feature magnitude of top k snippets from a video: $g_{\theta, k}(\mathrm{X}) = max_{\Omega_k(\mathrm{X}) \subseteq \{\mathrm{x}_t\}^T_{t = 1}}\frac{1}{k}\sum_{x_t \in \Omega_k(\mathrm{X})} ||x_t||_2$ (2), where $g_{\theta, k}(\cdot)$ is parameterised by $\theta$ to indicate its dependency on $s_\theta(\cdot)$ to produce x$_t, \Omega_k(\mathrm{X})$ containts a subset of $k$ snippets from {x$_t$}$^T_{t = 1}$ and |$\Omega_k(\mathrm{X})$| = $k$. The separability between abnormal and normal videos is defined: $d_{\theta, k}(\mathrm{X}^+, \mathrm{X}^-) = g_{\theta, k}(\mathrm{X}^+) - g_{\theta, k}(\mathrm{X}^-)$ (3). Probability that a snippet from $\Omega_k(\mathrm{X}^+)$ is abnormal with $p^+_k(\mathrm{X}^+)$ =  $\frac{min(\mu, k)}{k + \epsilon}$, with $\epsilon$ > 0 and from normal $\Omega_k(\mathrm{X}^-), p^+_k(\mathrm{X}^-) = 0$. 
<h3>4.2 Multi-scale temporal feature learning</h2>
MTN captures multi-resolution local temporal dependencies and global temporal dependencies between video snippets. It uses a pyramid of dilated convolutions over time domain to learn multi-scale representations fro video snippets. Dilated convolution is usually applied in the spatial domain with goal of expanding the receptive field without losing resolution. Use dilated convolutions as it is important to capture multi-scale temporal dependencies neighbouring video snippets for anomaly detection. MTN learns multi-scale temporal features from pre-computed features F = $[\mathrm{f}_d]^D_{d = 1}$. Then given feature $\mathrm{f}_d \in \mathbb{R}^T$, the 1-D dilated convolution operation with kernel $\mathrm{W}^{(l)}_{k, d} \in \mathbb{R}^W$ with $k \in {1,...D/4}, d \in {1,...,D}, l \in$ {PDC$_1$, PDC$_2$, PDC$_3$} and $W$ denoting filter size and is defined by: $\mathrm{f}^{(l)}_k = \sum_{d = 1}^D \mathrm{W}^{(l)}_{k, d} *^{(l)}\mathrm{f}_d$, where $*^{(l)}$ represents dilated convolution operator indexed by $l, \mathrm{f}^{(l)}_k \in \mathbb{R}^T$ represents output features after applying dilated convolution over temporal dimension. Dilation factors for {PDC$_1$, PDC$_2$, PDC$_3$} are {1, 2, 4}. Aim to produce an attention map $\mathrm{M} \in \mathbb{R}^{T \times T}$ that estimates pairwise correlation between snippets. TSA module first uses a 1 $\times$ 1 convolution to reduce spatial dimension from F $\in \mathbb{R}^{T \times D}$ to F$^{(c)} \in \mathbb{R}^{T \times D/4}$ with F$^{(c)}$ = $Conv_{1 \times 1}$(F). Then, apply three separate 1 $\times$ 1 convolution layers to F$^{(ci)}$ = $Conv_{1 \times 1}$(F$^{(c)}$) for $i \in$ {1, 2, 3}. The attention map is then built with M = (F$^{(c1)}$)(F$^{(c2)}$)$^T$, which produces F$^{(c4)}$ = $Conv_{1 \times 1}$(MF$^{(c3)}$). A skip connection is added after this final 1 $\times$ 1 convolution layer: F$^{(TSA)} = F^{(c4)} + F^{(c)}$. The output from MTN is formed with a concatenation of outputs from PDC and MTN modules $\overline{\mathrm{F}}$ = $[\mathrm{F}^{(l)}]_{l \in \mathcal{L}} \in \mathbb{R}^{T \times D}$, with $\mathcal{L}$ = {PDC$_1$, PDC$_2$, PDC$_3$, TSA}. A skip connection using original features F produces the final temporal feature representation X = $s_\theta$(F) = $\overline{\mathrm{F}}$ + F, where parameter $\theta$ comprises weights for all convolutions.
<h3>4.3 Feature magnitude learning</h3>
Propose a loss function to model $s_\theta$(F) in (1), where top $k$ largest snippet feature magnitudes from normal videos are minimised and top $k$ largest snippet feature magnitudes from abnormal videos are maximised. $l_s(s_\theta(\mathrm{F}_i), s_\theta(\mathrm{F}_j), y_i, y_j) = max(0, m - d_{\theta, k}(\mathrm{X}_i, \mathrm{X}_j))$ if $y_i = 1, y_j = 0$ and 0 otherwise, where $m$ is a pre-defined margin, X_$i$ = $s_\theta$(F$_i$) is the abnormal video feature and $d_{\theta, k}(\cdot)$ represents separability function defined in (3) that computes difference between score of top $k$ instances from $g_{\theta, k}(\cdot)$ in (2).
<h3>4.4 RTFM-enabled snippet classifier learning</h3>
Train a BCE-based classification loss function using set $\Omega_k$(X) that contains $k$ snippets with largest $l_2$-norm features from $s_\theta$(F) in (1). Loss: $l_f(f_\theta(s_\theta(\mathrm{F})), y) = \sum_{x \in \Omega_k(\mathrm{X})}-(y\log(f_\theta(\mathrm{x})) + (1 - y)\log(1 - f_\theta(\mathrm{x})))$, where $x = s_\theta$(f).
<h2>5. Datasets</h2>
ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds.
<h2>6. Metrics</h2>
AUC, AP.
<h2>7. Code</h2>
https://github.com/tianyu0207/RTFM.