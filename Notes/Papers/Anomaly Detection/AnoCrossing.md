<h2>1. Abstract</h2>
Formulate a new learning task: cross-domain few-shot anomaly detection, which can transfer knowledge learned from numerous videos in the source domain to help solve few-shot abnormality detection in the target domain. Concretely, leverage selfsupervised training on the target normal videos to reduce the domain gap and devise a meta context perception module to explore the video context of the event in the few-shot setting.
<h2>2. Method</h2>
<h3>2.1 Problem formulation</h3>
Given a target-domain dataset of sufficient normal samples and $K$ abnormal samples, denote all normal samples as a sub dataset $D^-_t$. Together with the abnormal samples, downsample $D^-_t$ to the same size $K$ to construct a 2-way-$K$-shot dataset $D_t = \{(x_t, y_t) \in X_t \times Y_t\}$ for few-shot learning. To enable the FSL, we include a source-domain dataset $D_s$ composed of sufficient labeled samples $(x_s, y_s) \in X_s \times Y_s$ to learn general knowledge for classification.
<h3>2.2 The anomaly crossing pipeline</h3>
First, source-domain video clip encoder for a good initialization of the model backbone. Then, normal samples are leveraged for anomaly detection in a new horizon that adapts the backbone learned from source domain to the target domain instead of training from scratch. Finally, the down-sampled normal samples paired with the few abnormal samples are used to train only the classification head, as known as a meta-testing stage.
- Training on the source domain: conduct supervised training on the source domain to learn a video encoder $\phi_s$ entailing the general knowledge for video classification. This process can be formalized as: $min_{\phi_s, c_s}\mathcal{L}_s(c_s(\phi_s(X_s)), Y_s)$, where $\mathcal{L}_s$ is the loss function for supervised training on the source domain, and $c_s$ is the classification head.
- Domain adaptation: devise a novel Domain Adaptation Module (DAM) to adapt the backbone parameter of $\phi_s$ to $\phi^-_t$ based on the normal samples $D^-_t$, so as to adapt representations on the source domain to the target domain. This process can be formalized as: $\phi^-_t = \mathcal{A}(\phi_s, D^-_t)$, where $\mathcal{A}$ refers to DAM.
- Meta-testing on the target domain: MCPM that can refine the input video clip features into a spatio-temporal contextualized video feature, and the whole parameter of MCPM are tuned in this stage. The parameter in MCPM $\phi_t$ constructs a mapping from $X_t$ to $f^v(X_t)$. Therefore, the objective of this process is: $f^c(x_t) = \{\phi^-_t(x_{t, i})\}, i = 1,...,n, phi_t(X_t) = f^v(X_t) = \mathcal{P}(f^c(X_t)), min_{\phi_t, c_t}\mathcal{L}_t(c_t(\phi_t(X_t)), Y_t)$, where $\mathcal{P}$ refers to the MCPM, $x_{t, i}$ refers $i_{th}$ sampled clip in video $x_t \in X_t, \mathcal{L}_t$ is the loss function for classification on target domain, and $c_t$ is the classification head.
<h3>2.3 Domain adaptation module</h3>
- Sample selections: . Given a video clip $c_0$, randomly crop three video clips $c_1, c_2, c_3$ and apply different transformations to them to build the data triplet containing anchor, positive, and negative samples for the later contrastive learning.
- Objective function: Contrastive learning is employed to learn the representations. Specifically, feed $a, p, n$ into $\phi_s$, get the clip feature $z_a, z_p, z_n$ and the final InfoNCE object function is: $\mathcal{L}_c = -\log\sum_{i = 1}^N\frac{\exp(z_{a_i} \cdot z_{p_i})}{sim(z_{a_i}, z_{p_i}, z_{n_i}) + \sum_{j = 0}^K\exp(z_{a_i} \cdot z_{a_j})}$, where $N$ is the numbers of video in $D^-_t$, $K$ is the number of other samples, and $sim(z_{a_i}, z_{p_i}, z_{n_i}) = \exp(z_{a_i} \cdot z_{p_i}) + \exp(z_{a_i} \cdot z_{n_i})$. As a result, $\phi_s$ will be adapted to $\phi^-_t$ via DAM.
<h3>2.4 Meta context perception module</h3>
Apply GCN in MCPM. A Semantic-Temporal Graph Convolution Network (STGCN) is chosen to capture the spatio-temporal information. For the video clip features fed into STGCN, sample $L$ clips $C = \{c_1,...,c_L\}$ with a fixed stride, then get video clip features $z_t = \phi^-_t(c_i)$ by the video clip encoder $\phi^-_t$. Then, build a video graph $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$, where $\mathcal{V} = \{z_l\}^L_{l = 1}$ and $\mathcal{E} = \mathcal{E}_t \cup \mathcal{E}_s$.
<h2>3. Datasets</h2>
DoTA and UCF-Crime.
<h2>4. Metrics</h2>
Accuracy.