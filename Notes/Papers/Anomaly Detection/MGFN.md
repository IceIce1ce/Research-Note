<h2>1. Abstract</h2>
Propose a novel glance and focus network to effectively integrate spatial-temporal information for accurate anomaly detection. Propose the Feature Am-
plification Mechanism and a Magnitude Contrastive Loss to enhance the discriminativeness of feature magnitudes for detecting anomalies.
<h2>2. Approach</h2>
<h3>2.1 Overview</h3>
First, feature extractor takes $B$ untrimmed videos V with video-level annotation as input, where $V_i \in \mathbb{R}^{N_i \times H \times W \times 3}$. Then, evenly segment each video sequence into $T$ clips and denote feature map from feature extractor as $F = \{f^{i, t}\}$, where $i \in [1, B], t \in [1, T] \in \mathbb{R}^{B \times T \times P \times C}$, where $P$ is the number of crops in each video clip and $C$ is feature dimension, $f^{i, t} \in \mathbb{R}^{P \times C}$ denotes feature of $t^{th}$ video clip in $V_i$. Taking feature map $F$ as input, FAM calculates feature norm $M$ to enhance $F$. Then, Glance Block and Focus Block integrate global and local features built upon video clip-level transformer (VCT) and self-attentional convolution (SAC). Design a Magnitude Contrastive (MC) loss to maximize the separability of normal features magnitudes and abnormal ones.
<h3>2.2 FAM</h3>
FAM first calculates feature norm $M^{i, t}$ of $f^{i, t}$ as: $M^{i, t} = (\sum_{c = 1}^C|f^{i, t, c}|^2)^{\frac{1}{2}} \in \mathbb{R}^{1 \times 1 \times P \times 1}$, where $c$ denotes the feature dimension index. Afterwards, FAM derives enhanced features $F_{FAM} = \{f^{i, t}_{FAM}\}$ by adding 1D-conv modulated feature norm, Conv1D($M^{i, t}$) to $f^{i, t}$ as a residue: $f^{i, t}_{FAM} = f^{i, t} + \alpha Conv1D(M^{i, t}) \in \mathbb{R}^{1 \times 1 \times P \times C}$.
<h3>2.3 Glance Block</h3>
First use a convolution to decrease the feature map dimension from $C$ in $F_{FAM}$ to $C/32$. After a short-cut conv (SCC) that outputs a feature map $F_{SCC\_GB} \in \mathbb{R}^{B \times T \times P \times C/32}$, construct a video clip-level transformer (VCT) to learn the global correlation among clips. Specifically, establish an attention map $A \in \mathbb{R}^{1 \times T \times T \times P}$ to correlate the different temporal clips. $A^{i, t_1, t_2} = \sum_{c = 1}^C Q(F^{i, t_1, c}_{scc\_GB})K(F^{i, t_2, c}_{scc\_GB})$, where $t_1, t_2 \in [1, T], Q, K$ are 1D query and key conv of transformer. Next, use softmax to generate $a \in \mathbb{R}^{1 \times T \times T \times P}$, where $a^{i, t_1, :}$ represents how other clips associate with clip $t_1$. $a^{i, t_1, t_2} = \frac{e^{A^{i, t_1, t_2}}}{\sum_{t_2 = 1}^Te^{A^{i, t_1, t_2}}}$. The output of VCT $F_{att\_GB} \in \mathbb{R}^{1 \times T \times P \times C/32}$ is the weighted average of all clips in the long video containing both normal and abnormal (if exists) ones: $F^{i, t_1, c}_{att\_GB} = \sum_{t_2 = 1}^Ta^{i, t_1, t_2}V(F^{i, t_2, c}_{scc\_GB})$, where $V$ is 1D value conv of transformer. Glance Block contains an additional Feed-Forward Network (FFN) including two fully-connected layers and a GeLU non-linear function to further improve the modelâ€™s
representation capability.
<h3>2.4 Focus Block</h3>
Focus Block (FB) consists of a short-cut convolution (SCC), a self-attentional convolution (SAC), and a Feed-Forward-Network (FFN). With $F_{GB}$ as
input, first increase the channel number to $C/16$ with a conv. Then SSC generates feature map $F_{SCC\_FB}$. Propose a self-attentional convolution (SAC) to enhance the feature learning in each video clip. Specifically, exploit $F_{scc\_FB}$ as both the feature map and convolution kernel, and formulate this step to be a convolution with kernel size=5 as: $F_{sac\_FB} = F_{scc\_FB} \times F_{scc\_FB} \in \mathbb{R}^{B \times T \times P \times C/16}$, where $F^{i, t, k_1}_{sac_FB} = \sum_{k_1, k_2 = 0}^{C/16}F^{i, t, k_1}_{scc\_FB}F^{i, t, k_2}_{scc\_FB} \in \mathbb{R}^{1 \times 1 \times P \times 1}$.
<h3>2.5 Loss functions</h3>
Magnitude constrastive loss: $L_{mc} = \sum_{p, q = 0}^{B/2}(1 - l)(D(M^p_n, M^q_n)) + \sum_{u, v = B/2}^B(1 - l)(D(M^u_a, M^v_a) + \sum_{p = 0}^{B/2}\sum_{u = B/2}^Bl(Margin - D(M^p_n, M^u_a))$, where $p, q$ are indexes of normal clips, and $u, v$ are for abnormal clips, $M_a$ indicates top-k feature magnitudes of abnormal clips and $M_n$ means those of normal clips, $D(\cdot, \cdot)$ is a distance metric, $l$ is an indicator function where $l = 1$ denotes a pair of normal and abnormal clips $p, u$ is sampled. $D(M^p_n, M^q_n) = min_{t = 0,...,T, r=0,...,T}(\mathbb{1}(||f^{p, t}_{FB}||_2) - \mathbb{1}(||f^{q, r}_{FB}||_2))$, where $\mathbb{1}$ is a top-k mean function. Similarly, distance $D(M^p, M^u)$ of normal feature $p$ and abnormal $u$ is defined: $D(M^p_n, M^u_a) = max_{t = 0,...,T, r=0,...,T}(\mathbb{1}(||f^{p, t}_{FB}||_2) - \mathbb{1}(||f^{u, r}_{FB}||_2))$. Total loss: $L = L_{sce} + \lambda_1L_{ts} + \lambda2L_{sp} + \lambda_3L_{mc}$, where $L_{ts}, L_{sp}$ denotes temporal smoothness loss and sparsity loss.
<h2>3. Datasets</h2>
UCF-Crime and XD-Violence.
<h2>4. Metrics</h2>
AUC.
<h2>5. Code</h2>
https://github.com/carolchenyx/MGFN.