<h2>1. Abstract</h2>
Begins with a relation-aware feature extractor to capture the multi-scale CNN features from a video. Afterwards, self-attention is integrated with CRFs to capture the short-range correlations of features and ability of CRFs in learning interdependencies of these features. Also, to deal with both local and non-local relationships of the features, a new variant of self-attention is developed by taking into consideration a set of cliques with different temporal localities. Moreover, a contrastive multi-instance learning scheme is considered to broaden the gap between the normal and abnormal instances.
<h2>2. Proposed method</h2>
<h3>2.1 Feature extraction</h3>
Modify TRN in two aspects: First, partition every frame into multiple scales; each of the partitioned image is referred to as an image patch. Depending on the partitioned scale, an image patch corresponds to either global or local features, which provide context and fine-grained appearance information. Second, make use of the inner-product operation to encode a sequence of data to unleash short-range correlations of image patches. Let the intermediate feature maps of the network be $D \in \mathbb{R}^{K \times W \times H \times C}$, the inner-product $\gamma(a, a') = \psi_1(d_a)^T\psi_2(d'_a)$ is used to decide short-term dependencies between positions $a$ and $a'$ in feature maps in neighboring frames, where $d_a, d'_a \in D$ denote features at position $a$ and $a'$ and $\psi_1, \psi_2$ are linear embedding layers. To reduce the complexity, apply global average pooling to image patches. These features are then concatenated into $B = [b_{1, 1},...,b_{K, G}]$, where $b_{i, j}$ denotes feature of image patch $i$ in frame $j$ and $G$ is total number of image patches in each frame.
<h3>2.2 Self-attention conditional random field</h3>
<h4>2.2.1 Spatial-temporal graph model</h4>
For a temporal window of $K$ frames, with $G$ image patches in each frame, establish a FC graph $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$. Each node ($i, j$) is related to a feature $b_{i, j} \in B$ derived from relation-aware feature extractor. The weight of the edge connecting node $(i, j)$ with node $(i', j'), p(b_{i, j}, b_{i', j'}) = exp(\theta(b_{i, j})^T\theta_2(b_{i', j'}))$, where $\theta_1, \theta_2$ are linear embedding functions.
<h4>2.2.2 Conditional random fields</h4>
Consider a set of node labels $\mathcal{X} = \{0, 1\}$ and a set of random variables $z = \{z_{1, 1},...,z_{G, K}\}$, where $z_{i, j}$ is a random variable associated with node $(i, j)$ and is assigned with a label in $\mathcal{X}$. For a fully-connected CRF model, the total energy can be written as: $E(z|B) = \sum_{i, j}\phi_u(z_{i, j}|b_{i, j}) + \sum_{i, j}\sum_{(i', j') \neq (i, j)}\phi_p(z_{i, j}, z_{i', j'} | b_{i, j}, b_{i', j'})$, where $\phi_u(z_{i, j}|b_{i, j})$ is unary energy, the cost of assigning a label to node $(i, j); \phi_p(z_{i, j}, z_{i', j'} | b_{i, j}, b_{i', j'})$ is pairwise energy, the cost of assigning labels to node $(i, j)$.
<h4>2.2.3 New self-attention</h4>
$h_{i, j} = \sum_{\tau \in \mathcal{K}}w_r\overline{h}^\tau_{i, j}$, where $\mathcal{K}$ is a prescribed set of temporal localities, $w_\tau$ is trainable scalar height and $\overline{h}^\tau_{i, j} = \sum_{\forall (i', j') \in C^\tau_j}p(b_{i, j}, b_{i', j'})\theta_3(b_{i, j})$ with $p(\cdot)$ is the pairwise similarity function.
<h4>2.2.4 Conditional random fields with self-attention</h4>
The total pairwise energy of assigning the same label to node $(i, j)$ can be modelled by: $e^p_{i, j} = \sum_{(i', j') \neq (i, j)}u(z_{i, j}, z_{i', j'})\hat{p}(b_{i, j}, b_{i', j'})f_p(e_3(b_{i, j}))$, where $f_p$ is a linear feed-forward layer. The total pairwise energy of all nodes can be represented as: $E_p = f_p(H)U$, where $H$ is self-attention outputs of all nodes and $U$ is symmetric matrix.
<h4>2.2.5 Mean-field inference</h4>
$W(z) = \prod_{i, j}W_{z_{i, j}}$, where $W_{z_{i, j}} = \frac{1}{Z_{i, j}}\exp(-(e^u_{i, j} + e^p_{i, j}))$, where $Z_{i, j}$ is normalization constant.
<h3>2.3 Contrastive multi-instance learning</h3>
$v_{i, j} = f_s(h_{i, j})$. The new loss function is given by: $\mathcal{L}_{total} = \mathcal{L}_{bn} + \alpha_1\mathcal{L}_{sp} + \alpha_2\mathcal{L}_{ts} + \mathcal{L}_{cs}$, where $\mathcal{L}_{bn}$ is BCE loss and $\mathcal{L}_{cs} = \frac{1}{(|B_p| \cdot |B_n|)^2}\sum_{(i, j) \in \mathcal{B}_p, (i', j') \in \mathcal{B}_n}||h_{i, j} - h_{i', j'}||^2_2$, where $|B_p|, |B_n|$ denote cardinality of positive and negative bags and $||h_{i, j} - h_{i', j'}||_2$ is Euclidean distance between two samples $h_{i, j}, h_{i', j'}$.
<h2>3. Datasets</h2>
UCF-Crime and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.