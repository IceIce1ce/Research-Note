<h2>1. Abstract</h2>
Design SSAGAN which is composed of the self-attentive predictor, the vanilla discriminator, and the self-supervised discriminator.
<h2>2. Methodology</h2>
<h3>2.1 Problem formulation</h3>
Given a set of rotational transformation $\mathcal{I} = \{\mathcal{R}(\hat{I}; r)\}^N_{r = 1}$ for each predicted frame $\hat{I}_t.$. The $t$th frame with $r$th rotation is denoted by $\hat{I}^r_t$ and $\hat{I}^r_t = \mathcal{R}(\hat{I}_t; r)$. The self-supervision discriminator $\mathcal{D}^S$ is trained to classify each rotated frame to one of the transformations, which can be expressed as: $\mathcal{D}^{S^*} = argmin_{\mathcal{D}^S}\frac{1}{N}\sum_{r = 1}^Nl(\mathcal{D}^S(\hat{I}^r_t), r)$, where $l$ indicates CE loss and $N$ = 4, $\hat{I}^r_t = \mathcal{R}(\hat{I}_t; r)$ indicates rotating frame $\hat{I}_t$ counterclockwise by $(r - 1) \cdot 90$ degree. As a result, SSAGAN can utilize both the prediction error and rotation detection error: $S(I_t) = ||I_t - \hat{I}_t||^2_2 + \frac{1}{N}\sum_{r = 1}^Nl(\mathcal{D}^S(\hat{I}^r_t), r)$.
<h3>2.2 Self-attentive frame predictor</h3>
It is composed of an encoder-decoder structure with skip connections for achieving multiscale feature fusion. A self-attention module is embedded into our frame predictor to capture the long-range contextual information.
<h4>2.2.1 Self-attention in P</h4>
Given a feature map $x \in \mathbb{R}^{C \times H \times W}$ from the former layer, it is first fed to two 1 $\times$ 1 conv layers to transform it into two corresponding feature space $g$ and $h$. Then, they are reshaped to $\mathbb{R}^{C \times K}$ where $K = H \times W$. Finally, the attention map $\mathcal{M} \in \mathbb{R}^{K \times K}$ is calculated by conducting matrix multiplication to $g(x)$ and $h(x)^T$. The attention matrix $\mathcal{M}$ is calculated via softmax. In addition, $x \in \mathbb{R}^{C \times H \times W}$ is also fed to another 1 $\times$ 1 conv layer to yield feature map $v(x) \in \mathbb{R}^{C \times K}$. Then, $v(x)$ is multiplied by $\mathcal{M}$ and reshaped to $\mathcal{R}^{C \times H \times W}$. Finally, multiply $\mathcal{M}$ by a scalar factor which is then added to $x$. The final output of self-attention module is: $\mathcal{O}_j = x_j + \beta \cdot \sum_{i = 1}^K\mathcal{M}_{j, i} \cdot v(x_i)$, where $i$ and $j$ are positions of maps.
<h4>2.2.2 Future frame prediction</h4>
Define a conditional loss $\mathcal{L}_C$ composed of three components: intensity loss $l_1$, gradient loss $l_G$ and motion loss $l_M$. Let $\mathcal{P}_\phi, \mathcal{D}^\mathcal{V}_A$ indicate the responses of frame predictor with parameter $\phi$ and vanilla discriminator with parameter $A$. Then, the intensity loss $l_1$ is: $l_1(\phi; t) = ||\mathcal{P}(I_{(t - \Delta t + 1):(t - 1)}) - I_t||^2_2$. The gradient loss $l_G$ is defined: $l_G(\phi; t) = \frac{1}{WH}\sum_{i = 1}^W\sum_{j = 1}^H|\bigtriangledown\hat{I}_t(i, j) - \bigtriangledown I_t(i, j)|$. The motion loss $l_M$ is calculated as: $l_M(\phi; t) = ||\mathcal{F}(\hat{I}_t, I_{t - 1}) - \mathcal{F}(I_t, I_{t - 1})||_1$. The total conditional loss $\mathcal{L}_C$ for the predictor is: $\mathcal{L}_C(\phi) = \sum_{t \in \mathcal{B}}\lambda_Il_I(\phi; t) + \lambda_Gl_G(\phi; t) + \lambda_Ml_M(\phi; t)$, where $\mathcal{B}$ indicates mini batch size. The generative adversarial loss is: $l^{\mathcal{D}^\mathcal{V}}_\mathcal{P}(\phi) = \sum_{t \in \mathcal{B}}-log(\mathcal{D}^{\mathcal{V}}_A(\hat{I}_t))$, where $\mathcal{D}^{\mathcal{V}}_A(\cdot)$ indicates probability that predicted frame $\hat{I}_t$ is identified as real by the vanilla discriminator. The loss function of vanilla discriminator is defined as: $l_{\mathcal{D}^\mathcal{V}}(A) = \sum_{t \in \mathcal{B}}-log(1 - \mathcal{D}^\mathcal{V}_A(\hat{I}_t)) - log(\mathcal{D}^\mathcal{V}_A(I_t))$.
<h3>2.3 Augmenting with self-supervision</h3>
The self-supervision adversarial loss of frame predictor $\mathcal{P}$ is defined: $l^{\mathcal{D}^S}_\mathcal{P}(\phi) = \sum_{t \in \mathcal{B}}\sum_{r \in \mathcal{R}}-log\mathcal{D}^S_A(\hat{I}^r_t)$, where $r \in \mathcal{R} = \{0, 90, 180, 270\}$ is the rotation angle. The self-supervision adversarial loss of self-supervision discriminator $\mathcal{D}^S$ is defined: $l_{\mathcal{D}^S}(A) = \sum_{t \in \mathcal{B}}\sum_{r \in \mathcal{R}}-log\mathcal{D}^S_A(I^r_t)$. The total adversarial loss of frame predictor $\mathcal{P}$ is: $\mathcal{L}^\mathcal{P}_A(\phi) = l^{\mathcal{D}^\mathcal{V}}_\mathcal{P}(\phi) + \alpha l^{\mathcal{D}^S}_\mathcal{P}(\phi)$. During the training phase, the overall loss of frame predictor $\mathcal{P}$ is: $\mathcal{L}_\mathcal{P}(\phi) = \mathcal{L}_C(\phi) + \lambda_A\mathcal{L}^\mathcal{P}_A(\phi)$. The total loss of discriminator can be expressed as: $\mathcal{L}_\mathcal{D}(A) = l_{\mathcal{D}^\mathcal{V}}(A) + \beta l_{\mathcal{D}^S}(A)$.
<h3>2.4 Anomaly detection</h3>
Apply PSNR to calculate the prediction error. The rotation detection error can be calculated as: $\mathcal{R}(t) = \frac{1}{N}\sum_{r = 1}^Nl(\mathcal{D}^S(\hat{I}^r_t), r)$, where $r \in \mathcal{R} = \{0, 90, 180, 270\}$ is the rotation angle and $N$ = 4. The final anomaly score for each frame is defined as: $S(t) = \eta\mathcal{N}(\mathcal{R}(t)) + (1 - \eta)(1 - \mathcal{N}(\mathcal{Y}(t)))$, where $\eta$ is weight factor for rotation detection error and $\mathcal{N}(\cdot)$ represents min-max normalization.
<h2>3. Datasets</h2>
UCSD Ped1, UCSD Ped2, CUHK Avenue and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.