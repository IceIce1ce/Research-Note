<h2>1. Abstract</h2>
TransAnomaly combines U-Net and ViViT to capture richer temporal information and more global contexts, To make full use of ViViT for prediction, modified the ViViT to make it capable of video prediction. Furthermore, model can perform anomaly localization by tracking the location of patches with lower regularity scores.
<h2>2. Proposed method</h2>
<h3>2.1 Encoder</h3>
The input of an encoder is a 3-channel image with a resolution of 256 $\times$ 256 and the output is a feature map of 512 channels with a resolution of 32 $\times$ 32. The activation functions for all convolutions in the encoders are ReLUs.
<h3>2.2 Transformer module</h3>
The temporal transformer receives $t$ feature maps output by the encoders, and outputs a predicted feature map. Firstly, the feature maps $x_1,...,x_t$ output by the generator are embedded into groups of tokens. Specifically, each feature map $x_i \in \mathbb{R}^{C \times H \times W}, x_i$ is reshaped into a sequence of flattened 2D patches $x_{p_i} \in \mathbb{R}^{N_p \times (p^2.C)}$, where $(P, P)$ is the resolution of a patch, and $N_p = HW/p^2$ is the number of patches. The constant latent vector size is set to $D$ in the temporal encoder, so the flattened patches are mapped to $D$ dimensions with a trainable linear projection. The projected flattened patches are denoted as $x'_{p_i, j}$, where $j = 1,...,N_p$ and $x'_{p_i, j}$ denotes the $j^{th}$ projected token in the sequence $x_{p_i}$. Tokens with the same $j$ are seen as a token group $[x'_{p_1, j},...,x'_{p_t, j}]$. An additional token $x_{pred_j}$ is added to each token group as the prediction token. Futhermore, standard 1D learnable temporal position embedding is applied to preserve temporal location information. $x_i \in \mathbb{R}^{512 \times 32 \times 32}, P$ is set to 2, $D$ is set to 512 and $t$ is set to 4. After the embedding, there are 256 groups of tokens. For each group, there are 5 tokens, and the lengths of the tokens are 512. The patch embedding can be described as follows: $z^{(0)}_j = [x_{pred_j}; x'_{p_1, j};...;x'_{p_t, j}] + E_{pos_j}$, where $x_{pred_j} \in \mathbb{R}^D$ denotes the prediction token of the $j^{th}$ token group and $E_{pos_j} \in \mathbb{R}^{(t + 1) \times D}$ denotes the temporal position embedding of the $j^{th}$ token group. $z^{(0)}_j$ denotes the input of the first layer of the temporal transformer. The temporal transformer encoder consists of $L_t$ layers of MSA and MLP blocks. The $N_p$ token groups are encoded by the temporal transformer encoder separately. The output of the $l^{th}$ layer of the temporal transformer encoder can be described as follows: $z'^{l - 1}_j = MSA(LN(z^{(l - 1)}_j)) + z^{(l - 1)}_j, z^{(l)}_j = MLP(LN(z'^{(l - 1)}_j)) + z'^{(l - 1)}_j$, where $z^{(l)}_j$ denotes the output of $l^{th}$ layer of the temporal transformer encoder, and $LN(\cdot)$  denotes layer normalization. After the encoding, $z^{(L_t)}_j$ is the final output of the temporal transformer encoder, and $z^{(L_t)}_{pred_j}$ is the prediction token of $z^{(L_t)}_j$, which is the predicted representation of the spatial position $j$. The output of the temporal transformer encoder consists of $N_p$ groups of tokens. The prediction tokens are then input into the spatial transformer, which encodes global information.
<h3>2.3 Decoder</h3>
The decoder receives a predicted feature map $\hat{x}_{t + 1}$ and outputs a predicted frame $\hat{I}_{t + 1}$. Similar to the original U-Net, the shortcuts between the encoders and the decoder suppress gradient vanishing, and more low-level features are leveraged in the upsampling process.
<h3>2.4 Constraints</h3>
Apply both appearance constraint and motion constraint. Intensity loss and gradient loss are adopted as appearance constraints. The intensity loss is: $L_{int}(I, \hat{I}) = \frac{1}{N}||I - \hat{I}||^2_2$, where $N$ is the number of the pixels in I, and the gradient loss is defined as follows: $L_{gd}(I, \hat{I}) = \frac{1}{N}\sum_{i, j}|||I_{i, j} - I_{i - 1, j}| - |\hat{I}_{i, j} - \hat{I}_{i - 1, j}|||_1 + |||I_{i, j} - I_{i, j - 1}| - |\hat{I}_{i, j} - \hat{I}_{i, j - 1}|||_1$, where $i, j$ denotes the spatial indexes of pixels. Instead of optical flow loss, adopt image difference loss as the motion constraint. The image difference loss is defined as follows: $L_{dif}(I_{t + 1}, \hat{I}_{t + 1}, I_{t + 2}, \hat{I}_{t + 2}) = \frac{1}{N}|||I_{t + 2} - I_{t + 1}| - |\hat{I}_{t + 2} - \hat{I}_{t + 1}|||^2_2$.
<h2>3. Datasets</h2>
Avenue and UCSD.
<h2>4. Metrics</h2>
AUC.