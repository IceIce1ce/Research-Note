<h2>1. Abstract</h2>
Propose a scale-aware weakly supervised learning approach to capture local and salient anomalous patterns from the background, using only coarse video-level labels as supervision. We achieve this by segmenting frames into nonoverlapping patches and then capturing inconsistencies among different regions through our patch spatial relation (PSR) module, which consists of self-attention mechanisms and dilated convolutions. A multi-scale patch aggregation method is further introduced to enable local-to-global spatial perception by merging features of patches with different scales. Considering the importance of temporal cues, we extend the relation modeling from the spatial domain to the spatio-temporal domain with the help of the existing video temporal relation network to effectively encode the spatio-temporal dynamics in the video.
<h2>2. Methodology</h2>
<h3>2.1 Overview</h3>
Divivide the video $V$ into a sequence of temporal non-overlapping video snippets $\{v_t\}^T_{t = 1}$, where $T$ denotes the number of video snippets. Given a video snippet $v_t \in \mathbb{R}^{H \times W \times L \times 3}$, it will first be split into several sets of spatial non-overlapping patch cubes with different spatial sizes. Set a number of sliding window sizes $\{(h_r, w_r)\}^R_{r = 1}$ to extract patch cubes. Thus, each set of patch cubes is represented as $\mathcal{P}^r_t = \{p^r_{t, i}\}^{N_r}_{i = 1}, r \in \{1,...,R\}$, where $p^r_{t, i} \in \mathbb{R}^{h_r \times w_r \times L \times 3}$ denotes the extracted patch cube with spatial patch size of $h_r \times w_r$ and $N_r = \lfloor H/h_r\rfloor \times \lfloor W/w_r\rfloor$ is the number of patch cubes. Then every patch cube is fed into I3D to generate features, and then features of patch cubes are stacked in horizontal dimension as $\mathcal{X}^r_t \in \mathbb{R}^{N_r \times D}$, where $D$ denotes the feature dimensions. To capture the inconsistencies between anomalous and normal patch cubes, PSR module computes patch-wise correlations among patch cubes of the same size through a self-attention mechanism and dilated convolutions to produce spatial enhanced patch representations, denoted as $\phi^r_t \in \mathbb{R}^{N_r \times D}$. After that, apply a multi-scale patch aggregation method to enhance the scale robustness of model. The features of the multi-scale patch cubes will be fused to produce an aggregated snippet feature $\phi'_t \in \mathbb{R}^D$, enabling a local-to-global perception of the video snippet. After the above process, obtain the aggregated video representation $\phi_A = [\phi'_t]^T_{t = 1}$, which is then fed into VTR module to capture the temporal dependencies among video snippets, resulting in a temporal enhanced video representation $\phi_{ST} = [\phi''_t]^T_{t = 1}$, where $\phi''_t \in \mathbb{R}^D$ denotes the enhanced feature of each snippet. Finally employ a snippet classifier to generate anomaly scores $\{s_t\}^T_{t = 1}$ for all video snippets.
<h3>2.2 Patch spatial relation module</h3>
PSR module learns the patch-wise correlations of patch cubes across different spatial regions from the pre-extracted initial patch features $\mathcal{X}^r_t \in \mathbb{R}^{N_r \times D}$, where $N_r$ denotes the number of patch cubes extracted with a particular spatial size $(h_r, w_r)$. After feeding $\mathcal{X}^r_t$ into PSR module, non-local network first uses a $1 \times 1$ convolution to reduce the channel dimension from $\mathcal{X}^r_t \in \mathbb{R}^{N_r \times D}$ to $\overline{\mathcal{X}}^r_t \in \mathbb{R}^{N_r \times D/4}$, then a non-local operation is performed to model global spatial relations among patch cubes. Then, calculate the attention map $M \in \mathbb{R}^{N_r \times N_r}$ by the dot product operation and softmax normalization. Then, obtain self-attention based representation $\hat{\mathcal{X}}^r_t \in \mathbb{R}^{N \times D/4}$ by a $1 \times 1$ convolution with learnable weights $W_z$ and a residual connection. Set up three 1-D dilated convolutions with different dilation factors $d \in \{1, 2, 4\}$. The input features $\mathcal{X}^r_t$ will be fed into three dilated convolutions to produce multi-scale dilation embedded representations $\mathcal{X}^T_{*, t} \in \mathbb{R}^{N \times D/4}, * \in \{DC1, DC2, DC3\}$. Then, a concatenation operation and residual connection are applied to the outputs of two sub-networks to produce the spatial enhanced patch representations: $\phi^r_t = [\hat{\mathcal{X}}^r_t, \mathcal{X}^r_{*, t}] + \mathcal{X}^r_t$.
<h3>2.3 Multi-scale patch aggregation</h3>
Each input patch representation $\phi^r_t$ will first be reconstructed into a 3-D feature vector $\phi^r_t \in \mathbb{R}^{\lfloor H/h_r\rfloor \times \lfloor W/w_r\rfloor \times D}$ according to the initial spatial location of patch cubes. Subsequent convolutional and fully connected layers transform this 3-D feature vector into a 1-D feature vector $\phi^r_t \in \mathbb{R}^D$. After the above steps, the local information of the same scale patches is aggregated. Finally, the multi-scale fused patch features $\{\phi^r_t\}^R_{r = 1}$ will be aggregated together by an element-wise add operation, resulting in an aggregated snippet feature $\phi'_t = \sum^R_{r = 1}\phi^r_t$.
<h3>2.4 Video temporal relation module</h3>
VTR module aims to learn the temporal context of video snippets by applying the relation network over the time dimension.
<h3>2.5 Loss function</h3>
For the output spatio-temporal video representation $\phi_{ST} = [\phi''_t]^T_{t = 1}$, where $\phi''_t \in \mathbb{R}^D$ denotes the snippet feature, mean feature magnitude is defined: $g(\phi_{ST}) = \mathrm{max}_{\Omega_k(\phi_{ST}) \subseteq \{\phi''_t\}^T_{t - 1}}\frac{1}{k}\sum_{\phi''_t \in \Omega_k(\phi_{ST})}||\phi''_t||_2$, where $\Omega_k(\phi_{ST})$ contains $k$ snippets selected from $\{\phi''_t\}^T_{t = 1}$, the snippet feature magnitude is computed by $l_2$ norm. After that, the feature magnitude based MIL ranking loss is formulated by: $\mathcal{L}_{FM} = \mathrm{max}(0, \epsilon - g(\phi^+_{ST}) + g(\phi^-_{ST}))$, where $\epsilon$ is a pre-defined margin, $\phi^+_{ST}, \phi^-_{ST}$ denote the anomaly and normal video representations. Feed the top $k$ selected snippets with largest feature magnitudes into the snippet classifier to generate the corresponding snippet anomaly scores $\{s_j\}^k_{j = 1}$. Then, apply a cross-entropy-base loss function to train the snippet classifier. Also add the sparsity and temporal smoothness constraints on all predicted snippet scores $\{s^+_t\}^T_{t = 1}$ of the anomaly video. The total loss function: $\mathcal{L} = \mathcal{L}_{CE} + \lambda_{fm}\mathcal{L}_{FM} + \lambda_1\sum_{t = 1}^T|s^+_t| + \lambda_2\sum_{t = 1}^T(s^+_t - s^+_{t - 1})^2$, where $\sum^T_{t = 1}|s^+_t|, \sum^T_{t = 1}(s^+_t - s^+_{t - 1})^2$ denote the sparsity regularization and temporal smoothness constraint.
<h2>3. Datasets</h2>
UCF-Crime and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.
<h2>5. Code</h2>
https://github.com/nutuniv/SSRL.