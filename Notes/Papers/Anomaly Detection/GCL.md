<h2>1. Abstract</h2>
Exploit the low frequency of anomalies towards building a cross-supervision between a generator and a discriminator. In essence, both networks get trained in a cooperative fashion, thereby allowing unsupervised learning.
<h2>2. Method</h2>
<h3>2.1 Training data organization</h3>
All input videos are arranged as segments, features of which are then extracted. Furthermore, these features are randomly arranged as batches. In each iteration a randomly selected batch is used to train the GCL model.
<h3>2.2 Generative cooperative learning</h3>
GCL consists of a generator $\mathcal{G}$ which is an AE and a discriminator $\mathcal{D}$ which is a FC classifier.
<h3>2.3 Generator network</h3>
$\mathcal{G}$ is trained by minimizing the reconstruction loss $\mathcal{L}_r$ as: $\mathcal{L}_r = \frac{1}{b}\sum_{q = 1}^b\mathcal{L}^q_G, \mathcal{L}^q_G = ||f^q_{i, j} - \hat{f}^q_{i, j}||_2$, where $f^q_{i, j}$ is a feature vector that is input to $\mathcal{G}$ and $\hat{f}^q_{i, j}$ is the corresponding reconstructed vector.
<h3>2.4 Pseudo labels from generator</h3>
$l^q_G = 1$ if $\mathcal{L}^q_G \geq \mathcal{L}^{th}_G$ else 0, where $\mathcal{L}^q_G$ is reconstruction loss and $\mathcal{L}^{th}_G$ is threshold.
<h3>2.5 Discriminator network</h3>
$\mathcal{L}_D \frac{-1}{b}\sum_{q = 1}^bl^q_G\mathrm{ln}\hat{l}^q_{i, j} + (1 - l^q_G)\mathrm{ln}(1 - \hat{l}^q_{i, j})$, where $l^q_G \in \{0, 1\}$ is the pseudo label generated by $\mathcal{G}$ and $\hat{l}^q_{i, j}$ is the output of $\mathcal{D}$ when a feature vector $f^q_{i, j}$ is input. 
<h3>2.6 Pseudo labels from discriminator</h3>
$l^q_D = 1$ if $\hat{p}^q_{i, j} \geq \mathcal{L}^{th}_D$ else 0, where $\mathcal{L}^{th}_D$ is computed the same way as the threshold $\mathcal{L}^{th}_G$ is computed.
<h3>2.7 Negative learning of generator network</h3>
Propose the following different types of pseudo targets: 1) All Ones Target: The original reconstruction target is replaced by a similar dimensional vector of all 1â€™s. 2) Random Normal Target: The original reconstruction target is replaced by a normal labeled feature vector selected arbitrarily. 3) Random Gaussian Noise Target: The original reconstruction target is perturbed by adding Gaussian noise. 4) No Negative Learning: No negative learning is applied to $\mathcal{G}$. $\mathcal{L}_G = \frac{1}{b}\sum_{q = 1}^b||t^q_{i, j} - \hat{f}^q_{i, j}||_2$, where the pseudo target $t_q$ is defined: $t^q_{i, j} = f^q_{i, j}$ if $l^q_D = 0$ else $1 \in \mathbb{R}^d$ if $l^q_D = 1$.
<h3>2.8 Anomaly scoring</h3>
Using reconstruction error of $\mathcal{G}$ or prediction scores of $\mathcal{D}$.
<h2>3. Datasets</h2>
UCF-Crime and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.