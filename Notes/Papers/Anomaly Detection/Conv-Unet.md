<h2>1. Abstract</h2>
The model is a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input.
<h2>2. Proposed method</h2>
The model includes two processing streams. The first one is performed via a Conv-AE to learn common appearance spatial structures in normal events. The second stream is to determine an association between each input pattern and its corresponding motion represented by an optical flow of 3 channels ($xy$ displacements and magnitude).
<h3>2.1 Inception module</h3>
Expect the model to early determine low-level features by putting the Inception module right after the input layer. Remove the max-pooling in this module since the input is a regular video frame instead of a collection of feature maps. Inception module includes 4 streams of convolutions of filter sizes $1 \times 1, 3 \times 3, 5 \times 5$ and $7 \times 7$. Each convolutional layer of filter larger than $1 \times 1$ is factorized into a sequence of layers with smaller receptive fields in order to reduce the computational cost.
<h3>2.2 Appearance convolutional autoencoder</h3>
Sub-network consists of the encoder and the top decoder without any skip connection. The encoder is constructed by a sequence of blocks including triple layers: convolution, batch-normalization (BatchNorm) and leakyReLU. The first block (right after the Inception module) does not contain BatchNorm layer. Instead of using pooling layer, apply strided convolution. The decoder is also a sequence of layer blocks. A dropout layer is attached before the ReLU in each block. The intensity loss is estimated as: $\mathcal{L}_{int}(I, \hat{I}) = ||I - \hat{I}||^2_2$. The gradient loss is defined: $\mathcal{L}_{grad}(I, \hat{I}) = \sum_{d \in \{x, y\}}|||g_d(I)| - |g_d(\hat{I})|||_1$, where $g_d$ denotes the image gradient along $d$-axis. The final loss is: $\mathcal{L}_{appe}(I, \hat{I}) = \mathcal{L}_{int}(I, \hat{I}) + \mathcal{L}_{grad}(I, \hat{I})$. 
<h3>2.3 Motion prediction U-Net</h3>
$\mathcal{L}_{flow}(F_t, \hat{F}_t) = ||F_t - \hat{F}_t||_1$, where $F_t$ is the ground truth optical flow estimated from two consecutive frames $I_t$ and $I_{t + 1}$ and $\hat{F}_t$ is the output of U-Net given $I_t$.
<h3>2.4 Additional motion-related objective function</h3>
Follow cGAN where both ground truth video frame and its corresponding optical flow are fed into discriminator. Given an input video frame $I$ and its associated optical flow $F$ obtained from FlowNet2, the proposed network produces a reconstructed frame $\hat{I}$ and a predicted optical flow $\hat{F}$ while discriminator $\mathcal{D}$ estimates a probability that OF associated to $I$ is the ground truth $F$. The GAN objective function consists of two loss functions: $\mathcal{L}_{\mathcal{D}}(I, F, \hat{F}) = \frac{1}{2}\sum_{x, y, c}-\log\mathcal{D}(I, F)_{x, y, c} + \frac{1}{2}\sum_{x, y, c}-\log[1 - \mathcal{D}(I, \hat{F})_{x, y, c}], \mathcal{L}_\mathcal{G}(I, \hat{I}, F, \hat{F}) = \lambda_\mathcal{G}\sum_{x, y, c}-\log\mathcal{D}(I, \hat{F})_{x, y, c} + \lambda_a\mathcal{L}_{appe}(I, \hat{I}) + \lambda_f\mathcal{L}_{flow}(F, \hat{F})$, where $x, y$ and $c$ indicate the spatial position and the corresponding channel of a unit in the feature maps outputted from $\mathcal{D}$ and $\lambda$ values are the weights associated to partial losses within proposed model. Assign 0.25 for $\lambda_\mathcal{G}$, 1 for $\lambda_a$ and 2 for $\lambda_f$.
<h3>2.5 Anomaly detection</h3>
Propose another score estimation scheme considering only a small patch instead of the entire frame. First, define partial scores individually estimated on the two model streams sharing the same patch position as $S_I(P) = \frac{1}{|P|}\sum_{i, j \in P}(I_{i, j} - \hat{I}_{i, j})^2, S_F(P) = \frac{1}{|P|}\sum_{i, j \in P}(F_{i, j} - \hat{F}_{i, j})^2$, where $P$ indicates an image patch and $|P|$ is its number of pixels. Frame-level score is then computed as: $S = \log[w_FS_F(\overline{P})] + \lambda_S\log[w_IS_I(\overline{P})]$, where $w_F$ and $w_I$ are the weights calculated according to the training data, $\lambda_S$ is to control the contribution of partial scores to the summation, and $\overline{P}$ is the patch providing the highest value of $S_F$ is the considering frame. The weights $w_F$ and $w_I$ are estimated as: $w_F = [\frac{1}{n}\sum_{i = 1}^nS_{F_i}(\overline{P}_i)]^{-1}, w_I = [\frac{1}{n}\sum_{i = 1}^nS_{I_i}(\overline{P}_i)]^{-1}$. Finally, perform a normalization on frame-level scores: $\hat{S}_t = \frac{S_t}{\mathrm{max}(S_{1..m})}$, where $t$ is the frame index in a video containing $m$ frames.
<h2>3. Datasets</h2>
CUHK Avenue, UCSD Ped2, Subway Entrance Gate and Exit Gate, Traffic-Belleview and Traffic-Train.
<h2>4. Metrics</h2>
AUC.
<h2>5. Code</h2>
https://github.com/nguyetn89/Anomaly_detection_ICCV2019.