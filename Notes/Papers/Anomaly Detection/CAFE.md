<h2>1. Abstract</h2>
With a pre-trained deep neural network (DNN) as teacher network, first feed raw video events into the teacher network and extract the outputs of multiple hidden layers as their high-level features. Guided by high-level features extracted from normal training videos, train a student network to be the highlevel feature extractor of normal events. For inference, a video event can be viewed as normal if the student extractor produces similar high-level features to the teacher network. Second, propose a consistency-aware scheme that requires high-level features extracted from neighboring frames to be consistent. Last, we also design a generic framework that can bridge high-level and low-level learning in VAD to further ameliorate VAD performance.
<h2>2. The proposed method</h2>
<h3>2.1 Preprocessing</h3>
first localize each foreground object by a bounding box on video frames. For each localized object on frames, we use its bounding box to crop a foreground patch. Finally, we resize the foreground patch to a fixed size $H \times W$ and regard the resized patch as a video event E, which serves as the basic processing unit in the proposed method.
<h3>2.2 Learning a high-level feature extractor</h3>
Given a video event E, first feed it into teacher network $\mathcal{T}$ and student network $S$. Denote $i$-th layer's outputs of $\mathcal{T}$ and $\mathcal{S}$ as $\mathcal{T}(E, i), S(E, i)$. Then, select certain high-level layers from $\mathcal{T}, S$ and build two ordered layer index set $K_\mathcal{T}$ and $K_S$. To enable $S$ to mimic $\mathcal{T}$, assume $K_\mathcal{T}$ shares the same cardinality with $K_S$. With $K_\mathcal{T}, K_S$, build 2 feature sets $F_\mathcal{T}(E), F_S(E)$. To encourage $S$ to be high-level feature extractor, maximize $max_S\sum^C_{j = 1}Sim(S(E, K^{(i)}_S), \mathcal{T}(E, K^{(i)}_\mathcal{T})$, where $Sim(\cdot)$ is a pre-defined similarity measure. MSE can be a highly effective similarity measure. The objective function is: $\mathcal{L}_{kd} = \sum_{j = 1}^C||S(E, K^{(i)}_S) - \mathcal{T}(E, K^{(j)}_\mathcal{T})||^2_2$.
<h3>2.3 Consistency-aware scheme</h3>
Given a video event E represented by a foreground patch, crop two patches by same location of E from 2 neighboring frames. Similarly, resize 2 patches into $H \times W$ to yield the context phases $C_{-1}, C_{+1} which serve as temporal context of E. Feed $C_{-1}, C_{+1}$ into student network $S$ to yield two feature set $F_S(C_{-1}), F_S(C_{+1})$. To encourage student network $S$ to assign consistent high-level features to patches $C_{-1}, E, C_{+1}$, maximize $\max_S\sum_{j = 1}^CCon(S(C_{-1}, K^{(j)}_S, S(E, K^{(i)}_S), S(C_{+1}, K^{(j)}_S)$, where $Con(x, y, z) = Sim(y, x) + Sim(y, z) + Sim(y, z)$. $\mathcal{L}_{ca} = \frac{1}{2}\sum_{j = 1}^C(||S(C_{-1}, K^{(j)}_S - S(E, K^{(j)}_S)||^2_2 + ||S(C_{+1}, K^{(j)}_S - S(E, K^{(j)}_S)||^2_2)$.
<h3>2.4 Bridging high/low-level learning</h3>
Given event E and its feature set produced by student network, train branch $\mathcal{D}$ to learn a pixel-level target G by taking high-level feature set $F_S(E)$ as input: $max_{\mathcal{D}}Sim(\mathcal{D}(F_S(E)), G)$. Explore 2 types of learning targets. First, integrate motion cues in videos, set learning target to be E's corresponding OF patch by $G = O(E)$ by using FlowNet 2. Based on E's location on original frame, crop an optical flow patch from the optical flow map and resize it into O(E) with the size $H \times W$. Second, choose raw video event to be learning target $G = E$. Simply select the last high-level feature $S(E, K^{(C)}_S)$ from entire feature set $F_S(E)$ to be input of $\mathcal{D}$. $\mathcal{L}_{lo} = ||\mathcal{D}(S(E, K^{(C)}_S)) - G||^2_2||$.
<h3>2.5 Training procedure</h3>
$\mathcal{L} = \mathcal{L}_{kd} + \lambda_{ca}\mathcal{L}_{ca} + \lambda_{lo}\mathcal{L}_{lo}$.
<h3>2.6 Anomaly inference</h3>
Design three types of anomaly scores to evaluate a video event E: distillation anomaly score, consistency based anomaly score and low-level learning based anomaly score.
<h2>3. Datasets</h2>
UCSDped2, Avenue and ShanghaiTech.
<h2>4. Metrics</h2>
AUC.