<h2>1. Abstract</h2>
Introduce the novel method AnomalyCLIP, the first to combine Large Language and Vision (LLV) models, such as CLIP, with multiple instance learning for joint video anomaly detection and classification. Also introduce a computationally efficient Transformer architecture to model short- and long-term temporal dependencies between frames.
<h2>2. Proposed approach</h2>
<h3>2.1 Selector model</h3>
Combine CLIP feature space and CoOp prompt learning approach to learn a set of directions in this space that identify each type of anomaly and their likelihood. Define normal prototype $m$ as the average feature extracted by the CLIP image encoder $\mathcal{E}_I$ on all $N$ frames $I$ contained in videos labelled as normal in dataset: $m = \frac{1}{N}\sum_{j = 1}^N\mathcal{E}_I(I_j)$. For each frame $I$ in the dataset, produce frame features $x$ by subtracting the normality prototype from CLIP encoded feature: $x = \mathcal{E}_I(I) - m$. Then exploit CLIP feature space and learn the textual prompt embeddings whose directions are used to indicate the anomalous classes. In particular, employ the prompt learning CoOp method to find such directions. Given a class $c$ and the textual description of the corresponding label $t^c$ expressed as a sequence of token embeddings, we consider a sequence of learnable context vectors $t^{ctx}$ and derive the corresponding direction for the class $d_c \in \mathbb{R}^D$ as: $d_c = \mathcal{E}_T([t^{ctx}, t^c]) - m$, where $\mathcal{E}_T$ indicates CLIP text encoder. The magnitude of the projection of frame feature $x$ on direction $d_c$ indicates likelihood of anomalous class $c: S(X) = [\mathcal{P}(x, d_1),...,\mathcal{P}(x, d_c)] \in \mathbb{R}^C$, where $\mathcal{P}$ indicates projection operation. Perform a batch norm after the projection which produces a distribution of projected features with zero mean and unitary variance: $\mathcal{P}(x, d_i) = BN(\frac{x \cdot d_i}{||d_i||})$. The definition of likelihood can be extended to segments by summing the likelihoods of each frame: $\mathcal{S}(S) = \sum_{i = 1}^F\mathcal{S}(x_i) \in \mathbb{R}^C$.
<h3>2.2 Temporal model</h3>
Implement $\mathcal{T}$ as Axial Transformer that computes attention separately for two axes corresponding to the segments and features in each segment. Terminate the model with a sigmoid activation so that output likelihood can also be interpreted as a probability.
<h3>2.3 Predictions aggregation</h3>
Combine predictions from $\mathcal{S}$ and $\mathcal{T}$ to obtain final output. Given an input frame feature $x$. The probability of frame being normal is $p_N(x) = 1 - p_A(x)$. To obtain the probability distribution of the frame to present an anomaly of a specific class $p_{A, c}(x)$, employ the predictions of the Selector model that can be seen as the conditional distribution over the anomalous classes $p_{c|A}(x) = softmax(\mathcal{S}(x))$. From the definition of conditional probability it follows that $p_{A, c}(x) = p_A(x) * p_{c|A}(x)$.
<h3>3. Datasets</h3>
ShanghaiTech, UCF-Crime and XD-Violence.
<h2>4. Metrics</h2>
AUC and mAUC.
<h2>5. Code</h2>
https://github.com/luca-zanella-dvl/AnomalyCLIP.