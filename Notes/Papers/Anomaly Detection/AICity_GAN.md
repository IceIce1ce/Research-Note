<h2>1. Abstract</h2>
Propose a semi-supervised method using Generative Adversarial Network trained on regular sequences to predict future frames. Our key idea is to model the ordinary world with a generative model, then compare a predicted frame with the real next frame to determine if an abnormal event occurs. We also propose a new idea of encoding motion descriptors and scaled intensity loss function to optimize GAN for fast-moving objects.
<h2>2. Introduction</h2>
Main contribution: 1) propose a new way of encoding the stacked motion descriptors into blended motion descriptor using an average image. Instead of feeding a sequence of ùëò frames into the generator network, use the current frame and the stacked motion frame as the two primary input data for the network; 2) propose to use Scaled Intensity Loss function for high quality fast-moving object focusing on the edges of objects, and weighted PSNR metric for scoring the anomaly events.
<h2>3. Proposed method</h2>
<h3>3.1 Future frame prediction</h3>
GAN-based U-Net model is trained by using the information extracted from the input frames together with information from the intensity, gradient, and flow between the last frame in the sequence and the next ground truth frame. This input strategy provides the motion direction by stacking subsequent frames and considers the last frame in the frame sequence as the main factor determining the detail of the next frame. Propose two new techniques to adapt the network to the traffic context: (i) to encode the input motion descriptor with average images and (ii) to refine the loss function to enhance the quality of predicted frames. Employ multiscale U-Net with 4 losses: L2 Loss (L2), Gradient Different Loss (GDL), Adversarial loss (Adv), Optical Flow Loss (OFL).
<h4>3.1.1 Encoded motion descriptor with average images</h4>
Instead of stacking the frames to show the motion trajectories, propose the encoded motion descriptor by using average images. From a given video, extract a sequence of average images $S = [avg_1, ..., avg_n]$, where $avg_i = (1 - \alpha) * avg_{i - 1} + \alpha * frame_i$,
<h4>3.1.2 Scaled intensity loss function</h4>
L2 intensity loss function is not good with fast-moving objects like vehicles. The scaled intensity loss is defined as $L_i = -\log(1 - sigmoid((I - I')^2))$.
<h3>3.2 Multiple anomaly events tracking</h3>
There are four main attributes in each anomaly proposal. The region attribute is used to localize the area where the potential anomaly event may occur. The starting time and frequency attributes represent the first time instant when this proposal is detected and the number of occurrences so far. Also save the information of the group of vehicles that are nearby to the region of an anomaly proposal to avoid missing anomaly events by occlusion.
<h3>3.3 Day-night detection</h3>
To represent the color, use HSV color space. Next, observe the different color distribution between many daytime periods. To reduce the noise caused by vehicle colors, only take the top half of an image. For night videos, the distribution of channelùëâ skews to the right. So, the image is night image if the total number of pixels that have Value channel from 0 to $lim_v$ larger than $n_v$, otherwise is a day. Define $F = {I_1, ..., I_n}$, where $F$ is a random set of frames in the video. The classification function for a single frame is: $S_i = \{x | x \in I_i, x_v < lim_v\}$ night($I_i$) = 1 if $|S_i| > n_v$ else 0, where $S_i$ is the set of satisfied pixels and $I_i$ is one frame in $F$. The final decision is the aggregation of night($I_i$) by majority votes: vote$_{\mathrm{night}} = \sum_1^n \mathrm{night}(I_i); \mathrm{vote_{\mathrm{day}}} - \sum_1^n 1 - \mathrm{night}(I_i)$, night(video) = true if vote$_{\mathrm{night}} > \mathrm{vote}_{\mathrm{day}}$ else false. This method needs two classifiers. The first one is for daytime classification for each single frame, and the second one is for daytime classification in the whole video.
<h2>4. Datasets</h2>
AI City 2019.
<h2>5. Metrics</h2>
$S = F1 \times (1 - NRMSE)$.