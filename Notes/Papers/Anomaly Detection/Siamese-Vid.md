<h2>1. Abstract</h2>
The main novelty is the idea of using a Siamese convolutional neural network (CNN) to learn a distance function between a pair of video patches (spatiotemporal regions of video). The learned distance function, which is not specific to the target video, is used to measure the distance between each video patch in the testing video and the video patches found in normal training video. If a testing video patch is not similar to any normal video patch then it must be anomalous.
<h2>2. Method</h2>
<h3>2.1 Generating training video patch pairs</h3>
A simple normalized L1 distance as our distance function along with the representation of video patches. It is possible for the L1 distance between two similar pairs to be larger than the L1 distance between two dissimilar pairs. Determine an adaptive threshold on normalized L1 distance below which to perform these pairings. An adaptive threshold for a given region in the camera frame is determined simply as $\mu + \alpha * \sigma$, where $\mu$ is the mean of nearest neighbor distances between testing video patches and training video patches, $\sigma$ is the corresponding standard deviation and $\alpha$ is determined by identifying an elbow in the distribution of nearest neighbor distances.
<h3>2.2 Learning a distance function</h3>
Choice of representation consists of a $H \times W \times C$ cuboid. In light of all anomalies being appearance or motion based, we adopt a multi-modal representation.
- Preprocessing: Data augmentation of a random amount is performed on every video patch pair $x_1, x_2$ during training in order to improve the robustness of the learned distance function to these variations. The data augmentation involves randomly flipping left to right, centrally scaling in [0.7, 1] and brightness jittering of the first channel in [-0.2, 0.2] in a stochastic manner on both video patches in a pair. Pre-processing also involves linearly scaling intensity values of each video patch from [0, 255] to [-1, 1].
- Network architecture and training: Each video patch in a pair is first processed independently using conv-relu-batchnorm operations with $2 \times 2$ max-pooling after every other convolution. Finally, flattened feature vectors from the two twin tails (conv5, conv5 5) are subtracted element-wise and processed consequently in a typical classification pipeline minimizing a cross-entropy loss. All convolutions use 3 Ã— 3 filters with a stride of 1. Let $B$ represent minibatch size, where $i$ indexes the minibatch and $y(x^{(1)}_1, x^{(i)}_2)$ be a length-$B$ vector which contains the labels for the mini-batch, where assume $y(x^{(1)}_1, x^{(i)}_2) = 0$ whenever $x_1$ and $x_2$ are similar video patches and otherwise. The crossentropy loss is of the form: $\mathcal{L}(x^{(i)}_1, x^{(i)}_2) = \gamma * y(x^{(i)}_1, x^{(i)}_2)\log p(x^{(i)}_1, x^{(i)}_2) - (1 - y(x^{(i)}_1, x^{(i)}_2))\log(1 - p(x^{(i)}_1, x^{(i)}_2))$, where $p(x^{(i)}_1, x^{(i)}_2)$ is the probability of the patches being dissimilar as output by the softmax function.
<h2>3. Datasets</h2>
UCSD Ped1/2 and CUHK Avenue.
<h2>4. Metrics</h2>
AUC and EER.
