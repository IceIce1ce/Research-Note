<h2>1. Abstract</h2>
First learn general representations of objects and their motions (using deep networks) and then use these representations to build a high-level, location-dependent model of any particular scene. This model can be used to detect anomalies in new videos of the same scene.
<h2>2. Method</h2>
<h3>2.1 High-level attribute learning</h3>
<h4>2.1.1 Appearance model</h4>
Select the following 8 categories as primary set of object classes : [Person, Car, Cyclist, Dog, Tree, House, Skyscraper, and Bridge]. Use labeled examples of each class (as well as background images containing none of the classes) taken from the CIFAR-10, CIFAR-100 and MIO-TCD datasets. In total, use 187793 RGB training examples resized to 64 $\times$ 64 pixels. Neural architecture: use a modified ResNext-50 network as backbone architecture. Modified the original model by adding an extra fully connected layer that maps the 2048-dimensional feature vector after the average pooling layer to a 128-dimensional layer. The 128- dimensional layer is then mapped by a final fully connected layer to an 8-dimensional output layer with sigmoid activations that represent the categories.
<h4>2.1.2 Motion model</h4>
Train deep networks to estimate the following attributes directly from an RGB video volume: (a) histogram of OF ($Y_{ang}$), (b) a vector of average speed of pixels in each direction of motion ($Y_{speed}$), (c) background classifier ($Y_{bkg, cls}$) and (d) percentage of stationary pixels ($Y_{bkg, pix}$). The histogram of optical flow consists of 12 bins each of which stores the fraction of pixels in the video volume that are estimated to be moving in one of the 30 degree directions of motion. The background classifier classifies whether the video volume contains motion or not.
- Motion training data: Obtain 283, 486 ‘background’ video volumes and 2, 551, 376 ‘motion’ video volumes. Use 90% of these for training our models and the remainder for validation. Represent $Y_{bkg, cls}$ attribute as a single binary variable denoting if a video volume is background or not. The ground truth for $Y_{ang}$ and $Y_{bkg}$ are computed by first computing a 13-bin normalized histogram, wherein the first 12 bins represent number of pixels with flow orientation in ranges $[i * \pi/6 : (i + 1) * \pi/6]$ with $i \in [0, 11]$, while the last bin denotes the number of pixels with flow magnitude below threshold. The histogram is then normalized by the total number of pixels. The first 12 bins of this histogram are used as the ground truth for $Y_{ang}$ and $13^{th}$ bin is the ground truth for $Y_{bkg}$. Finally, represent $Y_{mag}$ as a 12-dimensional vector denoting the average flow magnitude for pixels in each of the 12 flow orientation ranges.
- Learning task and neural architecture: the model have 3 layers of [3Dconv-BN-ReLU] followed by a fully connected layer. 
<h3>2.2 Model building</h3>
Use $F$ to denote a combined feature vector and app, ang, mag, and bkg to denote the appearance, angle, magnitude and background pixel fraction feature vectors, each of size $1 \times 128$. Finally, cls denotes the binary background classification of size $1 \times 1$. $F$ is of size $1 \times 513$. After computing features, use the exemplar selection approach to create a region-specific compact model of the nominal data. For each region, use the following greedy exemplar selection algorithm:
- Add the first feature vector to the exemplar set.
- For each subsequent feature vector, compute its distance to each feature vector in the exemplar set and add it to the exemplar set only if all distances are above a threshold.
To compute distance between two feature vectors, use $L_2$ distances. The distance function can be written as follows: $d_A(F_1, F_2) = ||A_1 - A_2||_2$, where $A \in \{app, ang, mag, bkg\}$. $d(F_1, F_2) = \frac{d_{app}}{Z_{app}} + \frac{d_{ang}}{Z_{ang}} + \frac{d_{bkg}}{Z_{bkg}} + \frac{d_{mag}}{Z_{mag}}$. The normalization factors $Z_{app}, Z_{ang}, Z_{mag}$ and $Z_{bkg}$ are computed once by finding the max $L_2$ distances between a large set of feature vector components computed from a validation set.
<h2>3. Datasets</h2>
UCSD Ped1/2, CUHK Avenue, Street Scene and ShanghaiTech.
<h2>4. Metrics</h2>
RBDC, TBDC and AUC.