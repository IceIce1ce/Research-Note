<h2>1. Abstract</h2>
Firstly, design the network of ML-MemAE-SC to memorize normal patterns for optical flow reconstruction. Then employ CVAE which captures the high correlation between video frame and optical flow, to predict the next frame given several previous frames.
<h2>2. Methodology</h2>
<h3>2.1 Multi-level memory-augmented autoencoder with skip connections</h3>
Design a four-level ML-MemAE-SC, including three encoding-decoding levels and the bottleneck. In each level of the encoder, stack two convolution blocks followed by a downsampling layer. In each level of the decoder, first copy the feature map from the encoder and then concatenate it with the upsampled feature maps of the lower level. The concatenation then sequentially passes through two convolution blocks, a memory module, and an upsampling layer. Downsampling and upsampling layers are implemented by convolution and deconvolution. For the memory modules, each memory module is a matrix $M \in \mathbb{R}^{N \times C}$. Each row of the matrix is called a slot $m_i$ with $i = 1,...,N$. To train ML-MemAE-SC, we can feed normal video, image or optical flow into it. Let $y$ be input data, $\hat{y}$ be the reconstructed result, $L_{recon} = ||y - \hat{y}||^2_2$. Add the entropy loss on the matching probabilities $\hat{w}_i$ for each memory module as: $\mathcal{L}_{ent} = \sum_{i = 1}^M\sum_{k = 1}^N-\hat{w}_{i, k}\log(\hat{w}_{i, k})$, where $M$ is the number of memory modules and $\hat{w}_{i, k}$ is the matching probabilities for $k$-th slot in $i$-th memory module. $mathcal{L}_{ML-MemAE-SC} = \lambda_{recon}\mathcal{L}_{recon} + \lambda_{ent}\mathcal{L}_{ent}$.
<h3>2.2 Conditional variational autoencoder for future frame prediction</h3>
Use CVAD as the generative model for modeling $p(x_{t + 1}|x_{1:t}, y_{1:t})$, in which we compute $z$ from $x_{1:t}$, then generate $x_{t+1}$ from $z$ with $y_{1:t}$ as the conditions. CVAD has two encoders $E_{\theta}$ and $F_{\phi}$ and one decoder $D_{\psi}$. $E_{\theta}$ encoders optical flows $y_{1:t}$ to obtain $E_{\theta}(y_{1:t})$. $F_{\phi}$ admits the concatenation of $x_{1:t}$ and $y_{1:t}$ and outputs features $F_{\phi}(x_{1:t}, y_{1:t})$. During training, sample $z$ from posterior distribution, and concatenate $z$ with conditions $E_{\theta}(y_{1:t})$ which are finally sent to the decoder to generate the future frame $\hat{x}_{t + 1}$. Also add skip connections between $F_{\phi}$ and $D_{\psi}$ to help generating $x_{t + 1}$. $\mathcal{L}_{CVAE} = KL[q(z|x_{1:t}, y_{1:t})||_p(z|y_{1:t})] + ||x_{t + 1} - \hat{x}_{t + 1}||^2_2$, where $x_{t + 1}$ is the ground truth future frame. Also define a gradient loss: $\mathcal{L}_{gd}(X, \hat{X}) = \sum_{i, j}||X_{i, j} - X_{i - 1, j}| - |\hat{X}_{i, j} - \hat{X}_{i - 1, j}|| ||X_{i, j} - X_{i, j - 1}| - |\hat{X}_{i, j} - \hat{X}_{i, j - 1}||$, where $i, j$ indicate spatial pixel position in an image. Train CVAE by: $\mathcal{L} = \lambda_{CVAE}L_{CVAE} + \lambda_{gd}\mathcal{gd}(\hat{x}_{t + 1}, x_{t + 1})$.
<h3>2.3 Anomaly detection</h3>
At test time, anomaly score is composed of two parts: (1) the flow reconstruction error as $S_r = ||\hat{y}_{1:t} - y_{1:t}||^2_2$ and (2) the future frame prediction error as $S_p = ||\hat{x}_{t + 1} - x_{t + 1}||^2_2$. Obtain the anomaly score by fusing the two errors: $S = w_r \cdot \frac{S_r - \mu_r}{\sigma_r} + w_p \cdot \frac{S_p - \mu_p}{\sigma_p}$, where $\mu_r, \sigma_r, \mu_p, \sigma_p$ are means and standard deviations of reconstruction errors and prediction errors of all the training samples, $w_r, w_p$ are the weights of the two scores.
<h2>3. Datasets</h2>
MNIST, UCSD Ped2, CUHK Avenue and SHTech.
<h2>4. Metrics</h2>
AUROC.
<h2>5. Code</h2>
https://github.com/LiUzHiAn/hf2vad.