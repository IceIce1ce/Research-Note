<h2>1. Abstract</h2>
Advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer while continuing to leverage the power of pre-trained image models. Action recognition: 84.9% top-1 accuracy on Kinetics-400 and 85.9% top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size and Temporal modeling: 69.6% top-1 accuracy on Something-Something v2.
<h2>2. Introduction</h2>
Present a pure-transformer backbone for video recognition that surpass the factorized models in efficiency. Taking advantage of the inherent spatiotemporal locality of videos, in which pixels that are closer to each other in spatiotemporal distance are more likely to be correlated. Because of this property, full spatiotemporal self-attention can be approximated by self-attention computed locally. Implement this approach through a spatiotemporal adaptation of Swin Transformer. Swin Transformer incorporates inductive bias for spatial locality as well as for hierarchy and translation invariance. Video Swin Transformer follows hierarchical structure but extends local attention computation from only the spatial domain to the spatiotemporal domain. As the local attention is computed on non-overlapping windows, the shift window mechanism of Swin Transformer is also reformulated to process spatiotemporal input.
<h2>3. Related Works</h2>
<h3>3.1 CNN and variants</h3>
For 3D modeling, C3D devises a 11-layer deep network with 3D convolutions. á»ˆD inflate 2D convolutions in Inception V1 to 3D convolutions with initialization by ImageNet pretrained weights. P3D, S3D and R(2+1)D disentangling spatial and temporal convolution leads to a speed-accuracy trade-off better than original 3D convolution.
<h3>3.2 Self-attention/Transformers to complement CNNs</h3>
NLNet adopts self-attention to model pixel-level long-range dependency for visual recognition tasks. CGNet presents an observation that the accuracy improvement of NLNet can mainly be ascribed to its global context modeling and it simplifies NL block into a lightweight global context block which matches NLNet in performance but with fewer params and less computation. DNL alleviate this degeneration problem by a disentangled design that allows learning of different contexts for different pixels while preserving shared global context.
<h3>3.3 Vision Transformers</h3>
DeiT integrates several training strategies that allow ViT to be effective using the smaller ImageNet-1K dataset. Swin Transformer introduces the inductive biases of locality, hierarchy and translation invariance, which enable it to serve as a general-purpose backbone for various image recognition tasks. VTN proposes to add a temporal attention encoder on top of pre-trained ViT, which yields good performance on video action recognition. TimeSformer suggests a factorized space-time attention for its strong speed-accuracy tradeoff. ViViT suggests an architecture similar to VTN. MViT is a multi-scale vision transformer for video recognition trained from scratch that reduces computation by pooling attention for spatiotemporal modeling.
<h2>4. Video Swin Transformer</h2>
<h3>4.1 Overall Architecture</h3>
Input video's size is $T \times H \times W \times 3$, consiting of $T$ frames which each contain $H \times W \times 3$ pixels. Treat each 3D patch of size 2 $\times$ 4 $\times$ 4 $\times$ 3 as a token. Thus, the 3D patch paritioning layers obtains $\frac{T}{2} \times \frac{H}{4} \times \frac{W}{4}$ 3D tokens, with each patch/token consisting of a 96-dimensional feature. A linear embedding layer is then applied to project features of each token to an arbitrary dimension C. Do not downsample along the temporal dimension because it allows us to follow hierarchical architecture of Swin Transformer which consists of 4 stages and performs 2x spatial downsampling in the patch merging layer of each stage. The patch merging layer concatenates features of each group of 2 $\times$ 2 spatially neighboring patches and applies a linear layer to project the concatenated features to half of their dimension. Major component of Video Swin Transformer is Video Swin Transformer block which is built by replacing multi-head self-attention (MSA) module in the standard Transformer layer with 3D shifted window based multi-head self-attention module and keeping the other components unchanged. Specifically, a video transformer block consits of a 3D shifted window based MSA module followed by a feed-forward network, specifically a 2-layer MLP with GELU in between. Layer Normalization (LN) is applied before each MSA module and FPN, and a residual connection is applied after each module.
<h3>4.2 3D Shifted Window based MSA Module</h3>
Introducing a locality inductive bias to self-attention module. 
<h4>4.2.1 Multi-head self-attention on non-overlapping 3D windows</h4>
Given a video composed of $T' \times H' \times W'$ 3D tokens and a 3D window size of $P \times M \times M$, the windows are arranged to evenly partition the video input in a non-overlapping manner. Input tokens are partitioned into $\lceil\frac{T'}{P}\rceil \times \lceil\frac{H'}{M}\rceil \times \lceil\frac{W'}{M}\rceil$ non-overlapping 3D windows. 
<h4>4.2.2 3D Shifted Windows</h4>
extend shifted 2D window mechanism of Swin Transformer to 3D windows for introducing cross-window connections while maintaining the efficient computation of non-overlapping window based self-attention. Given the number of input 3D tokens is $T' \times H' \times W'$ and size of each 3D window is $P \times M \times M$, for two consecutive layers, self-attention module is the first layer uses regular window partition strategy such that we obtain non-overlapping 3D windows. For self-attention module in the second layer, the window partition configuration is shifted along the temporal, height and width axes by ($\frac{P}{2}, \frac{M}{2}, \frac{M}{2})$ tokens from that of the preceding layer's self-attention module. Two consecutive Video Swin Transformer blocks are computed: 
- $\hat{z}^l = 3DW-MSA(LN(z^{l - 1})) + z^{l - 1}$,
- $z^l = FFN(LN(\hat{z}^{l})) + \hat{z}^{l}$,
- $\hat{z}^{l + 1} = 3DSW-MSA(LN(z^{l})) + z^{l}$,
- $z^{l + 1} = FFN(LN(\hat{z}^{l + 1})) + \hat{z}^{l + 1}$,
where $\hat{z}^{l}$ and $z^{l}$ denote the output features of 3D(S)W-MSA module and FPN module for block $l$, 3DW-MSA and 3DSW-MSA denote 3D window based multi-head self-attention using regular and shifted window partitioning configurations. This 3D shifted window design introduces connections between neighboring non-overlapping 3D windows in the previous layer.
<h4>4.2.3 3D Relative Position Bias</h4>
Introducing 3D relative position bias $B \in \mathbb{R}^{P^2 \times M^2 \times M^2}$ for each head as: Attention($Q, K, V$) = SoftMax($QK^T/\sqrt(d) + B)V$, where $Q, K, V \in \mathbb{R}^{PM^2 \times d}$ are query, key and value matrices, d is the dimension of query and key features and $PM^2$ is the number of tokens in a 3D window. Since relative position along each axis lies in range of $[-P + 1, P - 1]$ (temporal) or $[-M + 1, M - 1]$ (height or width), parameterize a smaller-sized bias matrix $\hat{B} \in \mathbb{R}^{(2P - 1) \times (2M - 1) \times (2M - 1)}$, and values in $B$ are taken from $\hat{B}$.
<h3>4.3 Architecture Variants</h3>
- Swin-T: C = 96, layer numbers = {2, 2, 6, 2}
- Swin-S: C = 96, layer numbers = {2, 2, 18, 2}
- Swin-B: C = 128, layer numbers = {2, 2, 18, 2}
- Swin-L: C = 192, layer numbers = {2, 2, 18, 2}
where C is channel number of hidden layers in first stage, window size $P = 8$ and $M = 7$. The query dimension of each head is $d$ = 32, and expansion layer of each MLP is $\alpha = 4$.
<h3>4.4 Initialization from Pre-trained Model</h3>
Compared to Swin Transformer, only two building blocks in Video Swin Transformers have different shapes, linear embedding layer in the first stage and relative position biases in Video Swin Transformer block. Input token is inflated to a temporal dimension of 2, thus the shape of linear embedding layer becomes 96 $\times$ C from 48 $\times$ C. Directly duplicate weights in pre-trained model twice and then multiply the whole matrix by 0.5 to keep mean and variance of output unchanged. The shape of relative position bias matrix is $(2P - 1, 2M - 1, 2M - 1)$, compared to $(2M - 1, 2M - 1)$ in Swin. To make relative position bias the same within each frame, duplicate matrix in pre-trained model $2P - 1$ times to obtain a shape of $(2P - 1, 2M - 1, 2M - 1)$ for initialization.
<h2>5. Datasets</h2>
Kinetics-400, Kinetics-600 and Something-Something v2.
<h2>6. Metrics</h2>
Top-1 and Top-5.
<h2>7. Code</h2>
https://github.com/haofanwang/video-swin-transformer-pytorch.