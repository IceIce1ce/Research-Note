<h2>1. Abstract</h2>
Video anomaly detection is often seen as one-class classification (OCC) problem. A popular way to tackle this problem is utilizing an autoencoder trained only on normal data. At test time, AE is then expected to reconstruct normal input well while reconstructing anomalies poorly. Propose a temporal pseudo anomaly synthesizer that generates fake-anomalies using only normal data. An AE is then trained to maximize the reconstruction loss on pseudo anomalies while minimizing this loss on normal data. This way, AE is encouraged to produce distinguishable reconstructions for normal and anomalous frames. 
<h2>2. Introduction</h2>
AEs can also often successfully reconstruct anomalous examples. In such cases, reconstruction loss between normal and anomalous data may not be discriminative enough to successfully identify the anomalies. Recently, a new addition to the field of OCC is the idea of utilizing pseudo anomalies generated from normal training data. Propose a simple yet highly effective temporal pseudo anomaly synthesizer to assist training of an AE in an end-to-end fashion without any bells and whistles. For each pseudo anomaly example as input, AE model is trained to produce high reconstruction loss. This helps in limiting the capability of AE to reconstruct anomalies at test time. Hypothesize that most of the anomalous events can be characterized by motion depicted. Propose a temporal pseudo anomaly synthesizer that injects synthesized anomaly examples into the training of an AE. To simulate anomalous movements from normal data, arbitrarily skip few frames to generate pseudo anomaly sequences. The overall training is then carried out to minimize the reconstruction loss of normal data while maximizing the reconstruction loss of synthesized anomaly data. The proposed method does not extract any handpicked motion information. Rather, by complementing a temporal pseudo anomaly synthesizer with a deep AP, we harness power of deep learning to detect a variety of anomalous activities inside videos.
<h2>3. Related work</h2>
Memory-based networks employ a memory mechanism over latent space between the encoder and the decoder of an AE. The network is restricted to use only the memorized normalcy definitions which limits its capability to reconstruct anomalous data. OGNet fuses normal images to generate appearance anomalies and train the network using both normal and fake anomaly examples. Moreover, OGNet requires a two-phase adversarial training scheme in which a discriminator is trained based on two previously frozen generator models.
<h2>4. Methodology</h2>
<h3>4.1 Architecture</h3>
Train AE as baseline, which takes a sequence of normal frames as input and produces its reconstruction as output. To complement the baseline training, propose a pseudo anomaly synthesizer that generates fake anomaly examples. These examples are then used for training with a probability $p$. This way, limit the reconstruction capability of AE by forcing it to increase the reconstruction loss on these fake anomaly examples. Finally, anomaly score is computed using frame-level reconstruction loss.
<h4>4.1.1 Autoencoder</h4>
AE model take $X$ as input of size $T \times C \times H \times W$, where $T, C, H$ and $W$ are number of frames, number of channels, height and width of frames. The reconstruction $\hat{X}$ is then given: $\hat{X} = \mathcal{D}(\mathcal{E}(X))$, where $\mathcal{E}$ and $\mathcal{D}$ are encoder and decoder of model. During training, a sequence of video frames $X$ is given as: $X = X_N$ with probability 1 - $p$ and $X = X_P$ with probability $p$, where $X_P$ is a sequence of frames generated using pseudo anomaly synthesizer, $X_N$ is a sequence of frames from normal training data and $p$ is probability that defines ratio of pseudo anomaly examples used.
<h4>4.1.2 Temporal pseudo anomaly synthesizer</h4>
Extract sequences of frames $X_N$ from a training video $V_i = {I_1, I_2,...,I_K}$ of length $K_i$ frames by randomly selecting a frame index $n$ from $V_i$ and then taking a fixed number of consecutive $T$ frames thereafter: $X_N = (I_n, I_{n + 1},...,I_{n + T - 1}) = (I_{n + t})_{0 \leq T, n + T - 1 \leq K_i}$. On the other hand, pseudo anomalies $X_P$ are synthesized by introducing a skip frame parameter $s$: $X_P = (I_n, I_{n + s},...,I_{n + (T - 1)s}) = (I_{n + ts})_{0 \leq T < T, n + (T - 1)s \leq K_i, s > 1}$. Skip frame parameter $s$ control the number of frames skip to generate temporal pseudo anomaly examples. 
<h3>4.2 Training</h3>
AE is trained on $X_N$ by minimizing reconstruction loss between input frame $I_{n + t}$ and its reconstruction $\hat{I}_{n + t}$ as follows: $L_N = \frac{1}{T \times C \times H \times W}\sum_{t = 0}^{T - 1}||\hat{I}_{n + t} - I_{n + t}||^2_F$. For $X_P$ generated by temporal pseudo anomaly synthesizer, loss is defined as: $L_P = -\frac{1}{T \times C \times H \times W}\sum_{t = 0}^{T - 1}||\hat{I}_{n + ts} - I_{n + ts}||^2_F$, where negative sign is introduced to increase the reconstruction loss of pseudo anomaly examples. This helps in limiting the reconstruction capability of AE on anomalous inputs. The overall loss $L$: $L = L_N$ if $X = X_N$ else $L = L_P$ if $X = X_P$. 
<h3>4.3 Anomaly score</h3>
First, compute PSNR $\mathcal{P}_t$ as: $\mathcal{P}_t = 10\log_{10}\frac{[M_{\hat{i}_t}]^2}{\frac{1}{R}||\hat{I}_t - I_t||^2_F}$, where $R$ is total number of pixels in $\hat{I}_t$, $t$ is the frame index and $M_{\hat{I}_t}$ is maximum possible value of $\hat{I}_t$. Then, normalize PSNR value to [0, 1] by min-max normalization: $\mathcal{Q}_t = \frac{\mathcal{P}_t - \mathrm{min}_t(\mathcal{P}_t)}{\mathrm{max}_t(\mathcal{P}_t) - \mathrm{min}_t(\mathcal{P}_t)}$, where $t$ is the frame index of $V_i$. The final anomaly score $\mathcal{A}_t$ is: $\mathcal{A}_t = 1 - \mathcal{Q}_t$.
<h2>5. Datasets</h2>
Ped2, Avenue, ShanghaiTech.
<h2>6. Metrics</h2>
AUC.
<h2>7. Code</h2>
https://github.com/aseuteurideu/STEAL. 