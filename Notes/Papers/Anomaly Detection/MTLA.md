<h2>1. Abstract</h2>
In one branch, motivated by the abilities of the semantic segmentation and future frame prediction tasks, combine them into a novel task of future semantic segmentation prediction to learn normal object classes and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, leverage optical flow magnitude estimation for motion anomaly detection and we propose an attention mechanism to engage context information in normal motion modeling and to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera.
<h2>2. Method</h2>
<h3>2.1 Multi-task learning</h3>
The first branch combines two different tasks (semantic segmentation and future frame prediction) to model appearance and motion simultaneously. The second branch is in charge of learning the correspondence between each normal object and its normal motion magnitude, with attention to its distance from the camera, motion direction, and its body parts.
<h3>2.2 The appearance-motion branch</h3>
This branch follows a teacherstudent strategy for anomaly detection. During training, a student (resnet34-UNet) gets two consecutive frames and learns to generate the semantic segmentation map of the future frame, assuming that at inference time, the prediction error would be higher for anomalies. The pseudo Ground-Truth (GT) for training the student network is generated by Mask-RCNN which is trained on MS COCO.
<h3>2.3 The motion branch</h3>
This branch also follows a teacher-student strategy where the student (a vgg16-UNet) learns to translate an input frame to its optical flow magnitude map, generated by a pre-trained optical flow extractor (considering its past frame) as a pseudo-GT. In motion branch, employ two different attention mechanisms. Employ a spatial and channel attention network in the main network, to dedicate more attention to special body parts (such as feet and hands) and also design a new attention network to help the network make predictions with attention to supplementary information (such as motion direction and relative distance information).
<h3>2.4 Spatial and channel attention</h3>
Apply the spatial and channel attention (SCSE) mechanism on the feature maps of mid-layers to help the network dedicate more attention to different body parts.
<h3>2.5 Attention to distance and direction</h3>
The network uses the direction and distance information as inputs and generates an attention map to apply to attentive layers. In the motion branch, the teacher network extracts the optical flow of two consecutive frames $(I_{t - 1}, I_t)$ and provides the magnitude of the optical flow as the pseudo-GT to be utilized by the student to map the input frame $(I_t)$ to its OFM. The generated OF features by the teacher encompasses the direction information in addition to magnitude information. Calculate the Cosine and Sine of Ang to normalize it and to generate two different features: motion parallel to the camera (X) and motion towards/away to/from the camera (Y), $Mag, Ang = OF(I_{t - 1}, I_t), X = |Cos(Ang)|, Y = |Sin(Ang)|$. Use MiDaS to estimate the relative depth maps of input frames. Use the hybrid version of it to balance precision and execution time.
<h3>2.6 Inference</h3>
Calculate the sum of activations in the anomaly map as the anomaly score S(t) of that branch for the frame: $S(t) = \sum|Out_{student}(I_t) - Out_{teacher}(I_t)|$, where $Out_{student}(I_t)$ and $Out_{teacher}(I_t)$ denote the estimations and expectations of a given branch. To relax the anomaly scores, Savitzkyâ€“Golay filter is applied on the anomaly scores: $S_r(t) = \frac{1}{N}\sum_{i = -w}^{i = w}\alpha^{S(t + i)}$, where $S_r(t)$ represents the relaxed anomaly score generated from noisy anomaly scores $S(t)$, $N$ is the normalizing factor and $\alpha$ and $w$ are the convolutional coefficients and window size. Finally, flag a frame as an anomaly if and only if the anomaly score $S(t)$ of either branch 1 or branch 2 (or both) is larger than a predefined threshold.
<h2>3. Datasets</h2>
ShanghaiTech and UCSD Ped 2.
<h2>4. Metrics</h2>
AUC.