<h2>1. Abstract</h2>
The proposed framework uses pre-trained deep models to extract high-level concept and context features for training denoising autoencoder (DAE). Furthermore, this framework presents the first video anomaly detection use of combing autoencoder and SHAP for model interpretability.
<h2>2. Proposed method</h2>
<h3>2.1 Framework design and proposed method</h3>
System is divided into three layers: Hardware, processing, and application layers.
<h3>2.2 Video anomaly detection</h3>
Denote a video as $\mathcal{O} = {\theta_1,...,\theta_n}$, where $\theta_i$ represents $i^{th}$ video frame and $i = 1,...,n$. Design a function $\mathcal{F}$ that firstly uses pre-trained CNN models to obtain the high-level concept and context features based on background segmentation, object classification, multi-object tracking, and semantic context information, and then, uses DAE with temporal denoising process to predict the video frame base on these features. The features are also used by DAE to explain anomalies through SHAP and video summary.
<h4>2.2.1 Feature extraction</h4>
To build the background segmentation feature, consider the Panoptic Feature Pyramid Network (PFPN). Only select semantic segmentation for background segmentation. The output can be written as: $L_{M_b \times N_b \times C_b} = F_{bg}(T)$, where for the input image at time $T$, the PFPN model $F_{bg}$ outputs a matrix with $C_b$ classified background labels as well as height $M_b$ and width $N_b$ information. Use JDE to get the pedestrian detection and tracking feature. The output is person tracking results, which can be written as: $p_{\hat{c}}, s_{\hat{c}}, v_{\hat{c}} = F_{ot}(\hat{c}, T)$, where $p_{\hat{c}}, s_{\hat{c}}, v_{\hat{c}}$ represent box coordinates, size and velocity of the person with ID $\hat{c}$. For the appearance feature, consider ResNet-101. The output of R101 model is written as $C_{K_{od}} = F_{od}(T)$.
<h4>2.2.2 Context mining</h4>
Classify the extracted contexts as spatial context, temporal context, and group context. Denote the mining spatial relationship process between different pre-trained models result as $R_{spatial}$. The spatial relationship including the intra-spatial relationship and the inter-spatial relationship. The intra-spatial relationship represents the inclusion result $S_1,...,S_C$ of regional classification $L$ with height $M_b$ and width $N_b$ and the $n_{ot}$ object detection/tracking results with coordinates $p_i, i = 1,...,n_{ot}$. The inter-spatial relationship consists of the adjacent object combinations. Use the following formula to represent the spatial relationship between object tracking and background segmentation: $O_{c_b . c_t} = R_{spatial}(F_{bg}, F_{ot})$. Denote the mining temporal relationship process among the pre-trained models result with timestamps as $R_{temporal}$. Consider the speed history of each person then update the Overspeed sign: $S_{Temp} = R_{temporal}(F_{ot})$, where $S_{Temp}$ is the frame-level Overspeed sign in the time range $T$. $R_{temporal}(F_{ot})$ denotes the relative relationship $R_{temporal}$ among the results of object tracking output $F_{ot}$. This feature smooths the speed measurement of the object tracking output. Consider the maximal average speed for each person and find the corresponding appearance in each frame. Finally, consider mining the group context $R_{group}(F_{ot})$ from object tracking features. It includes the min, max, and median value of the coordinates, and speed. Also use the sum of residuals in the least-squares solution of coordinates and speeds to measure the crowd sparsity.
<h4>2.2.3 Anomaly detection method</h4>
DAE is trained through reconstructing a clean input x by a corrupted input $\hat{x}$, where $\hat{x} =  x + s \cdot t$, $s$ is the noise factor and $t$ is the noise data distribution. In a basic one-layer DAE, the forward propagation for a basic AE with one hidden layer is: $h = Q(W^{(1)}\hat{x} + b_1), y = Q(W^{(2)}h + b_2)$. DAE uses sigmoid for each hidden layer and identity function for the output layer. As a single unit in DAE, its output is given by: $y_{NN}(x':w', b') = g(x', w' + b')$, where $w'$ is learned weight, $b'$ is learned bias and $x'$ is input. Add three fully connected hidden layers into DAE to form deep DAE. The layer nodes numbers are 50, 30, and 50. The code layer (The middle layer with the 30 nodes) stores the compressed representation space for the input features.
<h4>2.2.4 Temporal denoising</4h>
In temporal-denoising process, the reconstruction errors of a series of frames $e_1,...,e_n$ is filtered by Savitzky-Golay filter.
<h3>2.3 Video anomaly explanation</h3>
- Given the trained autoencoder and input instance, the features with top reconstruction errors are selected as the target output features.
- For each selected high error feature, fit the SHAP explainer with training background set, then use SHAP explainer to attribute the input features for predicting these high error features.
- The input features can be classified as contributing and offsetting features based on whether the reconstruction error is negative and positive.
<h2>3. Datasets</h2>
UCSD Ped1 and Ped2.
<h2>4. Metrics</h2>
AUC and EER.