<h2>1. Abstract</h2>
First, estimate feature similarity in videos recorded from multiple perspectives. Then, divide the video samples into high and low feature similarity groups. Next, extract spatio-temporal features from each group using two-branch DCNNs and fuse them using a rank-based weighted average pooling strategy followed by classification. Present a new road accident video dataset named MP-RAD, where each accident event is synthetically generated and captured from 5 independent camera perspectives using a computer gaming platform.
<h2>2. Proposed methodology</h2>
The proposed accident detection framework consists of three major modules. First, partition viewpoints into two subsets according to their feature similarity score. Second, two-branch DCNN extracts spatio-temporal features from each set. Third, fuse the obtained features using novel feature similarity-based weighted average pooling followed by classification. Assume an accident event ($\xi$) is being observed from a set of independent positions denoted by $[\alpha_1, \alpha_2,...,\alpha_\eta]$. Let the collection of videos representing the event $\xi$ be denoted by $\mathcal{X} = [V_{\alpha_i}]^T$, where $V_{\alpha_i}$ represents the video recorded from the $i^{th}$ position such that 1 $\leq i \leq \eta$. Define an objective function $\psi$ to calculate the feature similarity and a model hyper-parameter $k$ to partition the elements of $\mathcal{X}$ into two sets $\mathcal{X}H$ and $\mathcal{X}L$, such that $\mathcal{X} = \mathcal{X}H \cup \mathcal{X}L$. For each event $\xi$, a confusion matrix is constituted to assign a relative ranking of videos based on feature similarity score. In the next stage, a two-branch DCNN has been designed to extract spatio-temporal features from $\mathcal{X}H$ and $\mathcal{X}L$ sets. The final stage intelligently aggregates features using an ranked-based, weight-adaptive pooling strategy before being fed to a trainable classifier module.
<h3>2.1 Feature similarty-guided viewpoint partitioning</h3>
Feature similarity score for each viewpoint is calculated: $[F_{\alpha_i, \alpha_j}]^T = \psi(V_{\alpha_i; \alpha_j}), 1 \leq j \leq \eta$, where $[F_{\alpha_i, \alpha_j}]^T$ represents a $\eta$-dimensional feature similarity vector for angle $\alpha_i$ of the event and $\psi$ is the function to estimate feature similarity score between $\alpha_i$ and $\alpha_j$. ViSiL network is used to implement $\psi$. ViSiL utilizes fine-grained spatio-temporal correlation between a pair of videos, where intra-frame and inter-frame relations are preserved in better fashion. Estimate $\eta$ such feature similarity vectors, one for each angle/location. Thus, a confusion matrix  $\prod_{\eta \times \eta}$ is constituted. Each cell of $\prod$ represents a feature similarity score between [0, 1] with the diagonal elements being 1. The average feature similarity score is now calculated for each angle $(\alpha_i)$: $\overline{F}_{\alpha_i} = \frac{1}{\eta}\sum_{j = 1}^\eta F_{a_i, a_j}, \forall \alpha_i$. Now, a ranking of positions/angles is obtained based on $\overline{F}_{a_i}$. Higher the average feature similarity value, better the rank which is given: $R(\alpha_i) < R(\alpha_j)$, if $\overline{F}_{\alpha_i} > \overline{F}_{\alpha_j}$, where $R(\cdot)$ is rank of an angle/position with $i \ne j$. Introduce $k$, a hyper-parameter to partition $\mathcal{X}$ into $\mathcal{X}H$ and $\mathcal{X}L$: $[\mathcal{X}H, \mathcal{X}L] = \sigma(\prod, k)$, where $\sigma$ is partitioning function that partitions $\mathcal{X}$ into two sets with $|\mathcal{X}H| = k$ and $|\mathcal{X}L| = \eta - k$, and $k$ is a value between $[1:\eta]$. The set $\mathcal{X}H$ contains $k$ number of videos with higher relative feature similarity scores and $\mathcal{X}L$ contains remaining $\eta - k$ videos with lesser feature similarity.
<h3>2.2 Spatio-temporal feature extraction</h3>
Extract features using two separate branches denoted by models $(\phi^{\mathcal{X}H})$ and $(\Omega^{\mathcal{X}L})$. The first branch is dedicated to extract features for highly correlated viewpoints ($\mathcal{X}H$) and second one extracts features independently from $\mathcal{X}L$. The first branch consists of a series of $k$ number of identical 3D-CNNs that share weights in a similar way to T-C3D. The second branch consists of a series of $(\eta - k)$ number of 3D-CNNs without any weight sharing. Extract the spatio-temporal features using: $f^{\mathcal{X}H}_{\alpha_i} = A(\phi^{\mathcal{X}H}; W^H)_{\alpha_i \in [1...k]}, g^{\mathcal{X}L}_{\alpha_j} = B(\Omega^{\mathcal{X}L}; W^L)_{\alpha_j \in \{(\eta - k)...k\}}$, where $A$ represents feature extractor function applied on video $(\in \mathcal{X}H)$ recorded from the $i^{th}$ angle to generate feature-maps $f^{\mathcal{X}H}_{\alpha_i}$. Similarly, $B$ represents feature extractors function applied on video $(\in \mathcal{X}L)$ recorded from $j^{th}$ angle to generate feature-maps $g^{\mathcal{X}L}_{\alpha_j}$. Here, $W^H$ and $W^L$ are weights of $\phi^{\mathcal{X}H}$ and $\Omega^{\mathcal{X}L}$ such that 1 $\leq$ i $\leq$ $k$, and $\eta - k \leq j \leq \eta$. The obtained feature maps are then fed to pooling layers followed by feature aggregation. 
<h3>2.3 Pooling strategy and feature aggregation</h3>
<h4>2.3.1 Weighted average pooling</h2>
The input set $\mathcal{X}H$ is processed in parallel using $\phi^{\mathcal{X}H}$. The pooling function is presented using: $F_{\mathcal{X}H} = \frac{1}{k}\sum_{i = 1}^k \omega_i \cdot f^{\mathcal{X}H}_{\alpha_i}$, where $\omega_i$ is the weight for $i^{th}$ input such that $\sum_{i = 1}^k \omega_i = 1$ and $1 \leq i \leq k$. 
<h4>2.3.2 Average pooling</h4>
Adopt the standard average pooling in order to fuse outputs of $\Omega^{\mathcal{X}L}$ feature extractors as: $G_{\mathcal{X}L} = \frac{1}{\eta - k}\sum_{j = \eta - k}^\eta g^{\mathcal{X}L}_{\alpha_j}$. 
<h4>2.3.3 Aggregation function and classification</h4>
The feature extractors $\phi^{\mathcal{X}H}$ and $\Omega^{\mathcal{X}L}$ extract features of these temporal segments followed by respective pooling layers. The outputs of two separate pooling layers are then fused before feeding them to the classifier model.
<h2>3. Datasets</h2>
MP-RAD, UCF-Crime and CADP.
<h2>4. Metrics</h2>
AUC.
<h2>5. Code</h2>
https://github.com/draxler1/MP-RAD-Dataset-ITS-.