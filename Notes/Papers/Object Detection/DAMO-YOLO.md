<h2>1. Abstract</h2>
DAMO-YOLO is extended from YOLO with some new technologies, including neural architecture search (NAS), efficient reparameterized generalized-FPN (RepGFPN), a lightweight head with AlignedOTA label assignment and distillation enhancement. In particular, use MAE-NAS, a method guided by the principle of maximum entropy to search detection backbone under the constraints of low latency and high performance, producing ResNet-like/CSP-like structures with spatial pyramid pooling and focus modules. In the design of necks and heads, follow rule of "large neck, small head". Import Generalized-FPN with accelerated queen-fusion to build detector neck and upgrade its CSPNet with efficient layer aggregation networks (ELAN) and reparameterization. Then, investigate how detector head size affects performance and find that a heavy neck with only one task projection layer would yield better results. In addition, AlignedOTA is proposed to solve the misalignment problem in label assignment. And a distillation schema is introduced to improve performance to a higher level.
<h2>2. Introduction</h2>
In DAMO-YOLO, design a reparameterized generalized-FPN (RepGFPN). It is based on GFPN but involved in an accelerated queen-fusion, the efficient layer aggregation networks (ELAN) and re-parameterization. Propose AlignOTA to balance the importance of classification and regression. At last, knowledge distillation (KD) has been proved effective in boosting small models by larger model supervision.
<h2>3. DAMO-YOLO</h2>
<h3>3.1 MAE-NAS backbone</h3>
MAE-NAS constructs an alternative proxy based on information theory to rank initialized networks without training. Several basic search blocks are provided by MAE-NAS such as Mob-block, Res-block and CSP-block. The Mob-block is a variant of MobileNetV3 block, Res-block is derived from ResNet and CSP-block is derived from CSPNet.
<h3>3.2 Efficient RepGFPN</h3>
It consists of following insights: 1) due to the large difference in FLOPs from different scale feature maps, it is difficult to control same dimension of channels shared by each scale feature map under the constraint of limited computation cost. Therefore, in the feature fusion of neck, adopt setting of different scale feature maps with different dimensions of channels. 2) GFPN enhances feature interaction by queen-fusion, but it also brings lots of extra upsampling and downsampling operators. Therefore, under the constraints of real-time detection, remove extra upsampling operation in queen-fusion. 3) In the feature fusion block, first replace original 3 $\times$ 3 convolution-based feature fusion with CSPNet and obtain 4.2 mAP gain. Afterward, upgrade CSPNet by incorporating re-parameterization mechanism and connections of efficient layer aggregation networks (ELAN).
<h3>3.3 ZeroHead and AlignOTA</h3>
Discard the decoupled head but only left a task projection layer (one linear layer for classification and one linear layer for regression). In the loss after head, following GFocal, use quality focal loss for classification supervision and distribution focal loss and GIOU loss for regression supervision. QFL encourages to learn a joint representation of classification and localization quality. DFL provides more informative and precise bounding box estimations by modeling their locations as General distributions. The training loss of DAMO-YOLO is: $Loss = \alpha loss_{QFL} + \beta loss_{DFL} + \gamma loss_{GIOU}$. Introduce focal loss into classification cost and use IoU of prediction and ground truth box as soft label: $AssignCost = C_{reg} + C{cls}, \alpha = IoU(reg_{gt}, reg{pred}), C_{reg} = -ln(\alpha), C_{cls} = (\alpha - cls_{pred})^2 \times CE(cls_{pred}, \alpha)$
<h3>3.4 Distillation enhancement</h3>
Adopt feature-based distillation to transfer dark knowledge which can distill both recognition and localization information in intermediate feature maps $\rightarrow$ CWD is more fit with DAMO-YOLO. Distillation strategy is split into 2 stages: 1) teach distills student at first stage (284 epochs) on strong mosaic domain. 2) student finetunes itself on no mosaic domain at second stage (16 epochs). In DAMO-YOLO, the distillation is equipped with two advanced enhancements: 1) align module. On the one hand, it is a linear projection layer to adapt student feature's to the same resolution as teacher's. On the other hand, forcing student to approximate teacher feature directly leads to minor gains. 2) Channel-wise dynamic temperature, add a normalization to a teacher and student features to weaken effect the difference of real values brings. After subtracting the mean, standard deviation of each channel would function as temperature coefficient in KL loss.
<h2>4. Datasets</h2>
COCO and COCO + Obj365 + Open Image.
<h2>5. Metrics</h2>
AP, AP$^{50}$, AP$^{75}$, AP$^{S}$, AP$^{M}$ and AP$^{L}$.
<h2>6. Code</h2>
https://github.com/tinyvision/DAMO-YOLO.