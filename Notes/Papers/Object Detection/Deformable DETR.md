<h2>1. Abstract</h2>
propose Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference.
<h2>2. Introduction</h2>
DETR has 2 issues: 1) it requires much longer training epochs to converge and 2) DETR delivers relatively low performance at detecting small objects. Deformable DETR combines the best of the sparse spatial sampling of deformable convolution, and the relation modeling capability of Transformers. The module can be naturally extended to aggregating multi-scale features without FPN. In Deformable DETR, utilize (multi-scale) deformable attention modules to replace transformer attention modules processing feature maps. Explore a simple and effective iterative bounding box refinement mechanism to improve detection performance. Also try a two-stage Deformable DETR, where the region proposals are also generated by a variant of Deformable DETR which are further fed into decoder for iterative bounding box refinement.
<h2>3. Related work</h2>
<h3>3.1 Efficient attention mechanism</h3>
The first category is to use pre-defined sparse attention patterns on keys. The most straightforward paradigm is restricting the attention pattern to be fixed local windows. The second category is to learn data-dependent sparse attention. The third category is to explore low-rank property in self-attention.
<h3>3.2 Multi-scale feature representation for object detection</h3>
FPN, PANet, NAS-FPN, Auto-FPN and BiFPN.
<h2>4. Method</h2>
<h3>4.1 Deformable transformers for end-to-end object detection</h3>
<h4>4.1.1 Deformable attention module</h4>
The deformable attention module only attends to a small set of key sampling points around a reference point, regardless of the spatial size of feature maps. By assigning only a small fixed number of keys for each query, the issues of convergence and feature spatial resolution can be mitigated. Given an input feature map $x \in \mathbb{R}^{C \times H \times W}$, let $q$ index a query element with content feature $z_q$ and a 2-d reference point $p_q$, the deformable attention ferature is calculated: DeformAttn($z_q, p_q, x$) = $\sum_{m = 1}^M W_m[\sum_{k = 1}^K A_{mqk} \cdot W'_m x(p_q + \Delta p_{mqk})]$, where $m$ indexes the attention head, $k$ indexes the sampled keys and $K$ is the total sampled key number (K << HW). $\Delta p_{mqk}$ and $A_{mqk}$ denote the sampling offset and attention weight of the $k^{\mathrm{th}}$ sampling point in the $m^{\mathrm{th}}$ attention head. The scalar attention weight $A_{mqk}$ lies in [0, 1], normalized by $\sum_{k = 1}^K A_{mqk} = 1$. $\Delta p_{mqk} \in \mathbb{R}^2$ are of 2-d real numbers with unconstrained range. As $p_q + \Delta p_{mqk}$ is fractional, bilinear interpolation is applied in computing $x(p_q + \Delta p_{mqk})$. Both $\Delta p_{mqk}$ and $A_{mqk}$ are obtained via linear projection operator of 3$MK$ channels, where first 2$MK$ channels encode sampling offsets $\Delta p_{mqk}$ and the remaining $MK$ channels are fed to a softmax operator to obtain the attention weights $A_{mqk}$.
<h4>4.1.2 Multi-scale deformable attention module</h4>
Let $\{x^l\}^L_{l = 1}$ be the input multi-scale feature maps, where $x^l \in \mathbb{R}^{C \times H_l \times W_l}$. Let $\hat{p}_q \in [0, 1]^2$ be the normalized coordinates of the reference point for each query element $q$, then the multi-scale deformable attention module is applied as: MSDeformAttn($z_q, \hat{p}_q, \{x^l\}^L_{l = 1}$) = $\sum_{m = 1}^M W_m[\sum_{l = 1}^L \sum_{k = 1}^K A_{mlqk} \cdot W'_m x^l(\phi_l(\hat{p}_q) + \Delta p_{mlqk})]$, where $m$ indexes the attention head, $l$ indexes the input feature level and $k$ indexes the sampling point. $\Delta p_{mlqk}$ and $A_{mlqk}$ denote the sampling offset and attention weight of $k^{\mathrm{th}}$ sampling point in the $l^{\mathrm{th}}$ feature level and the $m^{\mathrm{th}}$ attention head. The scalar attention weight $A_{mlqk}$ is normalized by $\sum_{l = 1}^L \sum_{k = 1}^K A_{mlqk} = 1$. Here, use $\hat{p}_q \in [0, 1]^2$ for clarity of scale formulation in which the normalized coordinates (0, 0) and (1, 1) indicate the top-left and bottom-right image corners. Function $\phi_l(\hat{p}_q)$ re-scales the normalized coordinates $\hat{p}_q$ to the input feature map of $l$-th level. The proposed attention module will degenerate to deformable convolution when $L = 1, K = 1$ and $W'_m \in \mathbb{R}^{C_v \times C}$ is fixed as an identity matrix.
<h4>4.1.3 Deformable transformer encoder</h4>
Replace the transformer attention modules processing feature maps in DETR with proposed multi-scale deformable attention module. In encoder, extract multi-scale feature maps $\{x^l\}^{L - 1}_{l = 1} (L = 4)$ from the output feature maps of stages $C_3$ through $C_5$ in ResNet (transformed by a 1 $\times$ 1 convolution) where $C_l$ is of resolution 2$^l$ lower than input image. The lowest resolution feature map $x^L$ is obtained via a 3 $\times$ 3 stride 2 convolution on the final $C_5$ stage, denoted as $C_6$. All the multi-scale feature maps are of $C$ = 256 channels. For each query pixel, the reference point is itself. To identify which feature level each query pixel lies in, add a scale-level embedding, denoted as $e_l$, to the feature representation, in addition to the positional embedding. Different from the positional embedding with fixed encodings, the scale-level embedding $\{e_l\}^L_{l = 1}$ are randomly initialized jointly trained with the network.
<h4>4.1.4 Deformable transformer decoder</h4>
There are corss-attention and self-attention modules in the decoder. The query elements for both types of attention modules are of object queries. In the cross-attention modules, object queries extract features from the feature maps, where key elements are of the output feature maps from the encoder. In self-attention modules, object queries interact with each other, where key elements are of object queries. Only replace each cross-attention module to be the multi-scale deformable attention module, while leaving self-attention modules unchanged. For each object query, 2-d normalized coordinate of the reference point $\hat{p}_q$ is predicted from its object query embedding via a learnable linear projection followed by a sigmoid function.
<h3>4.2 Additional improvements and variants for deformable DETR</h3>
<h4>4.2.1 Iterative bounding box refinement</h4>
Each decoder layer refines the bounding boxes based on the predictions from previous layer.
<h4>4.2.2 Two-stage deformable DETR</h4>
Generate region proposals as the first stage. The generated region proposals will be fed into decoder as object queries for further refinement. In the first stage, each pixel in multi-scale feature maps would serve as an object query. Remove the decoder and form an encoder-only Deformable DETR for region proposal generation. In it, each pixel is assigned as an object query which directly predicts a bounding box. Top scoring bounding boxes are picked as region proposals. No NMS is applied before feeding the region proposals to second stage.
<h2>5. Datasets</h2>
COCO 2017.
<h2>6. Metrics</h2>
AP, AP$_{50}$, AP$_{75}$, AP$_{\mathrm{S}}$, AP$_{\mathrm{M}}$, AP$_{\mathrm{L}}$ and FPS.
<h2>7. Code</h2>
https://github.com/fundamentalvision/Deformable-DETR.