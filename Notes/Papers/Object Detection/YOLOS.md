<h2>1. Abstract</h2>
Propose YOLOS, a series of object detection models based on the vanilla vision transformer with the fewest possible modifications, region priors as well as inductive biases of the target task. 
<h2>2. Introduction</h2>
Present YOLOS, a series of object detection models based on ViT. 1) YOLOS replaces one [CLS] token for image classification in ViT with one hundred [DET] tokens for object detection. 2) YOLOS replaces image classification loss in ViT with the bipartite matching loss to perform object detection in a set prediction manner which can avoid re-interpreting the output sequences of ViT to 2D feature maps as well as prevent manually injecting heuristics and prior knowledge of object 2D spatial structure during label assignment.
<h2>3. You only look at one sequence</h2>
<h3>3.1 Architecture</h3>
YOLOS drops the [CLS] token for image classification and appends one hundred randomly initialized learnable detection tokens [DET] to the input patch embeddings [PATCH] tokens for object detection. During training, YOLOS replaces image classification loss in ViT with the bipartite matching loss to perform object detection in a set prediction manner.
<h3>3.2 Stem</h3>
Reshape the image x $\in \mathbb{R}^{H \times W \times C}$ into a sequence of flattened 2D image patches x$_{\mathrm{PATCH}} \in \mathbb{R}^{N \times (P^2 \cdot C)}$. Here, ($H, W$) is the resolution of input image, $C$ is the number of input channels, ($P, P$) is the resolution of each image patch and $N = \frac{HW}{P^2}$ is the resulting number of patches. Then, map x$_{\mathrm{PATCH}}$ to $D$ dimensions with a trainable linear projection E $\in \mathbb{R}^{(P^2 \cdot C) \times D}$. Refer to output of this projection x$_{\mathrm{PATCH}}$E as [PATCH] tokens. Meanwhile, one hundred randomly initialized learnable [DET] tokens x$_{\mathrm{DET}} \in \mathbb{R}^{100 \times D}$ are appended to [PATCH] tokens. Position embeddings P $\in \mathbb{R}^{(N + 100) \times D}$ are added to all input tokens to retain positional information. Use the standard learnable 1D position embeddings. the resulting sequence z$_0$ serves as input of YOLOS transformer encoder: z$_0 = [x^1_{\mathrm{PATCH}}\mathrm{E};...;x^N_{\mathrm{PATCH}}\mathrm{E}; x^1_{\mathrm{DET}};...;x^{100}_{\mathrm{DET}}]$ + P.
<h3>3.3 Body</h3>
It consists of a stack of transformer encoder layers only. [PATCH] tokens and [DET] tokens are treated equally and they perform global interactions inside transformer encoder layers. Each transformer encoder layer consists of one multi-head self-attention block and one MLP block. LayerNorm is applied before every block, and residual connections are applied after every block. MLP contains one hidden layer with GELU. Formally, for $l-th$ YOLOS transformer encoder layer: $\mathrm{z}'_l = \mathrm{MSA}(\mathrm{LN}(\mathrm{z}_{l - 1}) + \mathrm{z}_{l - 1}), \mathrm{z}_l = \mathrm{MLP}(\mathrm{LN}(\mathrm{z}'_l)) + \mathrm{z}'_l$.
<h3>3.4 Detector heads</h3>
Both classification and bounding box regression heads are implemented by one MLP with separate params containing two hidden layers with ReLU.
<h3>3.5 Detection token</h3>
Choose randomly initialized [DET] tokens as proxies for object representations to avoid inductive biases of 2D structure and prior knowledge about the task injected during label assignment. When fine-tuning on COCO, for each forward pass, an optimal bipartite matching between predictions generated by [DET] tokens and ground truth objects is established.
<h3>3.6 Fine-tuning at higher resolution</h3>
All params are initialized from ImageNet-1K except for MLP heads for classification & bounding box regression as well as one hundred [DET] tokens which are randomly initialized. Keep the patch size $P$ unchanged: $P \times P = 16 \times 16$, which results in a larger effective sequence length.
<h3>3.7 Inductive bias</h3>
Inductive biases inherent from ViT come from patch extraction at network stem part as well as resolution adjustment for position embeddings. Apart from that, YOLOS adds no non-degenerated convolutions upon ViT. From the representation learning perspective, choose to use [DET] tokens to bind objects for final predictions to avoid additional 2D inductive biases as well as task-specific heuristics.
<h2>4. Datasets</h2>
COCO, ImageNet-1K.
<h2>5. Metrics</h2>
Top-1, AP, FPS.
<h2>6. Code</h2>
https://github.com/hustvl/YOLOS.