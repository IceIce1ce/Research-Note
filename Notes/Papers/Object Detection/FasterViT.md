<h2>1. Introduction</h2>
Design a new family of hybrid CNN-ViT neural networks named FasterViT with a focus on high image throughput. FasterViT combines benefits of fast local representation learning in CNNs and global modeling properties in ViT. Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. Benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable efficient cross-window communication at lower costs. HAT can be used as a plug-and-play module for existing networks and enhance them.
<h2>2. Introduction</h2>
FasterViT consists of 4 different stages in which the input image resolution is reduced by using a strided convolutional layer while doubling the number of feature maps. Leverage residual convolutional blocks in high-resolution stages of the architecture (stage 1, 2) while employing transformer blocks in later stages (stage 3, 4). This strategy allows for fast generation of high-level tokens which can be further processed with transformer-based blocks. For each transformer block, use an interleaved pattern of local and propose HAT to capture both short and long-range spatial dependencies and efficiently model cross-window interactions. Hierarchical Attention learns carrier tokens as a summary of each local window and efficiently models the cross-interaction between these regions. The computational complexity of hierarchical attention grows almost linearly with input image resolution. 
<h2>3. Related work</h2>
<h3>3.1 Vision transformers</h3>
LeViT, T2T-ViT, CSwin, GCViT.
<h3>3.2 Towards enhanced efficiency</h3>
Efficient attention, network compression, dynamic interence, operator adaptation, token merging and manipulations, EfficientFormer, VisFormer and CrossViT.
<h3>3.3 Global self-attention</h3>
BigBird, LongFormer, EdgeViT, Twins and Focal Transformer.
<h2>4. FasterViT</h2>
<h3>4.1 Architecture</h3>
FasterViT exploits convolutional layers in the earlier stages that operate on higher resolution. The second half of the model relies on novel hierarchical attention layers to reason spatially across the entire feature maps. First half of the network and downsampling blocks make use of dense convolutional kernels. Also avoid squeeze-and-excitation operators and minimize Layer Normalization for higher resolution stages (1, 2) as these layers tend to be math-limited. Later stages (3, 4) in the architecture tend to be math-limited as GPU hardware spends more time on compute compared to memory transfer cost. As a result, applying multi-head attention will not be a bottleneck.
<h3>4.2 FasterViT Components</h3>
<h4>4.2.1 Stem</h4>
An input image x $\in \mathbb{R}^{H \times W \times 3}$ is converted into overlapping patches by two consecutive 3 $\times$ 3 convolutional layers, each with a stride of 2, which project them into a $D$-dimensional embedding. The embedded tokens are further batch-normalized and use ReLU after each convolution.
<h4>4.2.2 Downsampler Blocks</h4>
The spatial resolution is reduced by 2 between stages by a downsampling block. Apply 2D layer normalization on spatial features, followed by a convolutional layer with a kernel of 3 $\times$ 3 and a stride of two.
<h4>4.2.3 Conv Blocks</h4>
Stage 1 and 2 consist of residual convolutional blocks which are defined: $\hat{x} = GELU(BN(Conv_{3 \times 3}(x)))$, x = BN(Conv$_{3 \times 3}(\hat{x}))$ + x.
<h4>4.2.4 Hierarchical attention</h4>
Start with local windows in Swin Transformer. Then, introduce a notion of carrier tokens (CTs) that play the summarizing role of entire local window. The first attention block is applied on CTs to summarize and pass global information. Then, local window tokens and CTs are concatenated such that every local window has access only to its own set of CTs. By performing self attention on concatenated tokens, facilitate local and global information exchange at reduced cost. By alternating sub-global (CTs) and local (windowed) self-attention, formulate a concept of hierarchical attention. Conceptually, CTs can be further grouped into windows and have a higher order of carrier tokens. Given an input feature map x $\in \mathbb{R}^{H \times W \times d}$ in which $H, W$ and $d$ denote height, width and number of feature maps. Set $H = W$, first partition input feature map into $n \times n$ local windows with $n = \frac{H^2}{k^2}$, where $k$ is the window size as: $\hat{\mathrm{x}}_1$ = Split$_{k \times k}$(x). Initialize CTs by pooling to $L = 2^c$ tokens per window: $\hat{\mathrm{x}}_c$ = Conv$_{3 \times 3}$(x), $\hat{\mathrm{x}}_{ct}$ = AvgPool$_{H^2 \rightarrow n^2L}(\hat{\mathrm{x}}_c)$, where Conv$_{3 \times 3}$ represents efficient positional encoding, $\hat{\mathrm{x}}_{ct}$ and AvgPool denote carrier tokens and feature pooling operation, c is set to 1. These pooled tokens represent a summary of their respective local windows, set $L$ << $k$. The procedure of CT initialization is performed only once for every resolution stage. Every local window $\hat{\mathrm{x}}_1$ has unique set of carrier tokens, $\hat{\mathrm{x}}_{ct, 1}$, such that $\hat{\mathrm{x}}_{ct} = \{\hat{\mathrm{x}}_{ct, l}\}^n_{l = 0}$. In every HAT block, CTs undergo attention procedure: $\hat{\mathrm{x}}_{ct} = \hat{\mathrm{x}}_{ct} + \gamma_1 \cdot \mathrm{MHSA}(\mathrm{LN}(\hat{\mathrm{x}}_{ct})), \hat{\mathrm{x}}_{ct} = \hat{\mathrm{x}}_{ct} + \gamma_2 \cdot \mathrm{MLP}_{d \rightarrow 4d \rightarrow d}(\mathrm{LN}(\hat{\mathrm{x}}_{ct}))$, where MHSA represents multi-head self attention, $\gamma$ is a learnable per-channel scale multiplier, MLP is a 2-layer MLP with GeLU. Next, in order to model short-long-range spatial information, compute interaction between local and carrier tokens, $\hat{\mathrm{x}}_1$ and $\hat{\mathrm{x}}_{ct, 1}$. First, local features and CTs are concatenated. Each local window only has access to its corresponding CTs: $\hat{\mathrm{x}}_w = \mathrm{Concat}(\hat{\mathrm{x}}_1, \hat{\mathrm{x}}_{ct, 1})$. These tokens undergo another set of attention procedure: $\hat{\mathrm{x}}_w = \hat{\mathrm{x}}_w + \gamma_1 \cdot \mathrm{MHSA}(\mathrm{LN}(\hat{\mathrm{x}}_w)), \hat{\mathrm{x}}_w = \hat{\mathrm{x}}_w + \gamma_2 \cdot \mathrm{MLP}_{d \rightarrow 4d \rightarrow d}(\mathrm{LN}(\hat{\mathrm{x}}_w))$. Finally, tokens are further split back and used in the subsequent hierarchical attention layers: $\hat{\mathrm{x}}_1, \hat{\mathrm{x}}_{ct, 1}$ = Split($\hat{\mathrm{x}}_w)$. To further facilitate long-shot-range interaction, perform global information propagation in the end of stage. Finally, output of stage is computed as: x = Upsample$_{n^2L \rightarrow H^2}(\hat{\mathrm{x}}_{ct, 1}) + \mathrm{Merge}_{n^2 k^2 \rightarrow H^2}(\hat{\mathrm{x}}_1)$. Add absolute positional bias directly to CTs and local window tokens. Then, employ a 2-layer MLP to embed absolute 2D token location into feature dimension. Then, to facilitate image-like locality inductive bias, enhance attention with log space relative positional bias from SwinV2.
<h2>5. Datasets</h2>
ImageNet-1k, ImageNet-21K, MS COCO, ADEK.
<h2>6. Metrics</h2>
Top-1, AP$^{\mathrm{box}}$, AP$^{\mathrm{mask}}$.
<h2>7. Code</h2>
https://github.com/NVlabs/FasterViT.