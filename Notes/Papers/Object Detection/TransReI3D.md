<h2>1. Abstract</h2>
Propose novel task of damaged object re-identification, which aims at distinguishing changes in visual appearance due to deformations or missing parts from subtle intra-class variations. BB-Bicycles contains 39200 images and 2800 unique bike instances spanning 20 different bike models. Propose TransReI3D, a multi-task transformer-based deep network unifying damage detection (framed as a multi-label classification task) with object re-identification.
<h2>2. Introduction</h2>
Existing ReID benchmarks typically focusing on persons and vehicles are limited in size and variety. Long-term ReID requires ability to distinguish stable properties over time to account. Propose the novel task of damaged object re-identification which aims to identify same object in multiple images even in the presence of breaks, deformations and missing parts.
<h2>3. Related work</h2>
<h3>3.1 Transformer-based re-identification</h3>
Reranking Transformer. Compared to CNNs, transformers are better suited to handling long-range dependencies and avoid use of downsampling operators (pooling and strided convolutions) that may obscure important visual details.
<h3>3.2 Synthetic data in deep learning</h3>
Popular approaches for collecting synthetic data also include use of video games or fusing real and virtual data via compositing techniques and placing. One of the main challenges associated with synthetic data is domain shift between real and synthetic images, which can be tackled through transfer learning or domain adaptation. Domain randomization is a technique used to enhance variability of synthetic data.
<h2>4. Dataset</h2>
<h3>4.1 CGI pipeline</h3>
CGI pipeline consists of two main phases. The first phase is model preparation which is mostly manual and performed once for each bike model. In the second phase, a semi-automatic script generates a set of rendered images, depicting multiple views and variations of a given input bike, along with labeling and segmentation information. Damages and deformations were implemented based on classical CGI technique of 3D polygonal meshes armature deformation.
<h4>4.1.1 Model preparation</h4>
Input can be either 3D parametric model (CAD file) or a polygonal mesh. In the former case, a polygonal conversion is first required to generate a polygonal model. To ensure visually plausible deformations, a retopology operation has to be performed in order to obtain quad-flow based topologies with proper vertex density in the parts that will later be subject to deformation. Afterwards, model is rigged and skinned. To make model easily controlled, define a template rig that needs to be adapted to the given bike model. The template rig is made up of an armature, lattices and rail guides. The template Armature includes three groups/layers of bones. The red bones are linked to the seat and handlebar meshes. The green bones, placed in the salient parts/joints are used as inverse kinematic controls by the blue chains. The latter are so-called deformation bones was modified by adding/removing bones, if required by the peculiarities of bike model. These deformation chains are the ones used for bike frame mesh skinning, whereas other parts (seat, handlebars and wheels) are parented to the dedicated bones of other two groups. A set of predefined deformations were devised in the form of a pose library to both change the pose of movable bike components and introduce damages while rendering the images. Lattices were used to damage the wheels. A set of deformations was devised also in this case in the form of a shape key library. Additionally, Rail guides were used to break the bike frame exploiting a boolean mesh operation on a plane that takes guides as reference. 
<h4>4.1.2 Domain randomization and image rendering</h4>
After 3D model is arranged, it is possible to automatically render a variety of different pictures. First, 3D model is configured by randomly selecting a set of materials (texture, color and decals) from the material library. A physically based rendering material library was defined from which to pick a suitable material among several possible choices. A given combination of 3D model and materials corresponds to a single bicycle instance and is therefore assigned a unique ID. Second, for each ID, multiple images are generated, before and after a damage occurs, by applying the following transformations: 1) changing the pose of mobile parts, 2) applying mud or rust, 3) damage simulation, 4) point of view selection and 5) background and lighting selection. Possible damages include removal of one or more parts of bike (seat, pedals, handlebar and wheels), bent frame, broken frame and wheel deformation. Finally, rendered bike must be placed onto a suitable background, adjusting for the specific lighting conditions.
<h3>4.2 BBBicycles characteristics</h3>
<h4>4.2.1 Dataset distribution</h4>
It contains a total of 39200 images from 2800 unique IDs (20 models, 140 IDs each). 20 models retrieved from dedicated marketplaces were prepared, including 6 MTBs, 1 Enduro, 6 Road bikes, 1 Circuit, 1 Gravel and 5 Cruiser. For textures, collected five patterns of various styles. Additionally, 10 different decals containing logos from famous bike brands such as Bianchi and Cannondale were randomly applied. The background was selected from a pool of 11 different 360 degrees HDRIs, varying bike positioning and illumination by rotating camera. For each bike ID, up to 14 renderings were generated, evenly divided in before and after images. For before images, only dirt or rust was applied with 20% probability. For after images, dirt/rust was applied with 50% probability, damages to the frame were applied with 75% probability (25% were bent, 25% were broken and 25% were both bent and broken), and finally each removable part (seat, pedals, handlebar and wheels) was removed (50% probability) or deformed (50% probability).
<h4>4.2.2 Training, validation and stress test set</h4>
Training set contains 25676 images (1834 IDs, 14 models), validation set contains 1128 images (564 IDs, 12 models) and stress test contains 840 images (420 IDs, 3 models).
<h4>4.2.3 Real dataset</h4>
Combine a subset of DelftBikes. A total of 6292 images were collected, of which 106 presented a Bent (64) or Broken (52) frame.
<h2>5. Methodology</h2>
<h3>5.1 Problem setting</h3>
Assume that training set $D$ consists of $N$ sequences of synthetic images $D = {(x^1_i,...,x^M_i)}^N_{i = 1}$, where all images $x^j_i$ in a sequence are associated with same ID $i$ and represent same bike instance. Additionally assume that each image is associated with a set of binary attributes, each representing the presence of a specific kind of damage ($a^j_i \in \mathcal{A} = {BD, BK, P_n})$; $P_n$ indicates whether $n^{th}$ part is present or missing. Given $D$, learn an embedding space $x^j_i \in \mathbb{R}^{h \times w \times ch} \longmapsto e^j_i \in \mathbb{R}^m$ such that all images associated with a given ID $i$ are closer in the embedding space than other IDs. At inference time, a query image is compared against gallery and correct ID must be retrieved on basis of embedding distance.
<h3>5.2 TransReI3D architecture</h3>
TransReI3D builds on TransReID architecture. TransReID architecture builds on ViT architecture but includes additional components to capture more robust and fine-grained features. Specifically, Side Information Embedding module encodes non-visual information such as camera or viewpoint and is input to a transformer encoder together with learnable patch and position embeddings. The global ReID branch and Jigsaw branch then jointly learn ReID task, encoding global ($f_g$) and local ($f_t$) features. The Jigsaw branch is based on Jigsaw Patch Module which shuffles all patches and regroups them into several groups, all of which are input to a shared transformer layer to learn local features $f_l$.
<h3>5.3 Damage branch and multi-task learning</h3>
Multi-task learning is implemented using one shared transformer backbone and an additional separate transformer layer of each task. DD branch is a multi-label classifier with seven output heads, two for Bent and Broken frame labels and five for missing parts (front wheel, rear wheel, seat, handlebar or pedals). Each output head takes as input the $[cls]$ token and passes it through a BN layer followed by a FC layer. TransReI3D combines two tasks, one executed on image pairs (ReID) and one executed on individual images (DD). A multi-task diversion mechanism was implemented which selects the tasks that need to be executed upon extracted features of each training batch. Hence, synthetic images are forwarded to all branches, whereas real images are directed to DD branch only. Loss: $\mathcal{L} = \alpha\mathcal{L}_{ID}(f_g) + \beta\mathcal{L}_T(f_g) + \gamma\mathcal{L}_{\mathcal{D}}(f^a_g, f^p_g, f^n_g) + \frac{1}{k}\sum_{j = 1}^k(\mathcal{L}_{ID}(f^j_l) + \mathcal{L}_T(f^j_l))$, where $\mathcal{L}_T$ and $\mathcal{L}_{ID}$ are triplet loss and ID cross entropy loss, $\mathcal{L}_D$ is DD loss and k (= 4) is the number of classification heads of JPM branch. All loss components are calculated on $[cls]$ token. To compute $\mathcal{L}_T$, triplets are online sampled from each batch with hard negative and positive mining. $\mathcal{L}_D$ is a weighted binary cross-entropy loss: $\mathcal{L}_D = \lambda\mathcal{L}_{BD}(\cdot) + \mu\mathcal{L}_{BK}(\cdot) + \upsilon\frac{1}{n}\sum_{j = 1}^n(\mathcal{L}_{P_n}(\cdot))$, where $\mathcal{L}_{BD}$ and $\mathcal{L}_{BK}$ refer to Bent and Broken frame labels losses, and $\mathcal{L}_{P_n}$ to n = 5 specific missing parts losses.
<h3>5.4 Domain adaptation</h3>
TransReI3D is trained on BBBicycles and tested on real dataset without adaptation or fine-tuning. For supervised domain adaptation, leveraged multi-task training strategy to train model on real and synthetic data. For unsupervised adaption, experimented with domain adversarial technique DANN and partial domain adaptation PADA.
<h2>6. Datasets</h2>
BBBicycles.
<h2>7. Metrics</h2>
mAP, CMC, AUROC.